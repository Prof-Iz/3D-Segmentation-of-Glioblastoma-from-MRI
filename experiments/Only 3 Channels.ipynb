{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ibrah\\miniconda3\\envs\\fyp\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from os import path, listdir\n",
    "import os\n",
    "from monai.data import (Dataset, list_data_collate, DataLoader, decollate_batch, PersistentDataset)\n",
    "from monai.transforms import (\n",
    "    LoadImaged,\n",
    "    Compose,\n",
    "    MapTransform,\n",
    "    Orientationd,\n",
    "    ToMetaTensord,  # ? same as ensuretyped?\n",
    "    EnsureChannelFirstd,\n",
    "    NormalizeIntensityd,\n",
    "    Spacingd,\n",
    "    RandCropByPosNegLabeld,\n",
    "    RandFlipd,\n",
    "    RandScaleIntensityd,\n",
    "    RandShiftIntensityd,\n",
    "    Activations,\n",
    "    AsDiscrete,\n",
    ")\n",
    "\n",
    "from monai.networks.nets import  UNet\n",
    "from monai.losses import  DiceCELoss, DiceFocalLoss, GeneralizedDiceFocalLoss\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.optimizers import Novograd, WarmupCosineSchedule\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import torch\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "def encode_lgg_hgg(x):\n",
    "    \"\"\"Encode LGG and HGG as 0 and 1 for stratification\n",
    "\n",
    "    Args:\n",
    "        x (str): LGG or HGG in Dataframe\n",
    "\n",
    "    Returns:\n",
    "        int: encodes 0 for LGG and 1 for HGG\n",
    "    \"\"\"\n",
    "    return 0 if x == \"LGG\" else 1\n",
    "\n",
    "\n",
    "def train_val_test_dataset(data_path: str):\n",
    "    \"\"\"From 100% Cases take 20% cases as Validation.\n",
    "    Take the remaining 80% cases as training\n",
    "\n",
    "    Stratification done on data to ensure that the classes are balanced.\n",
    "\n",
    "    Args:\n",
    "        data_path (str, optional): Path to Name Mapping File provided by BraTS.\n",
    "\n",
    "    Returns:\n",
    "        training, validation, testing: list of case names split into training, validation and testing.\n",
    "    \"\"\"\n",
    "    data = pd.read_csv(data_path)\n",
    "    data = data[[\"Grade\", \"BraTS_2020_subject_ID\"]]\n",
    "    data.Grade = data[\"Grade\"].map(encode_lgg_hgg)\n",
    "    (training, validation, train_check, val_check,) = train_test_split(\n",
    "        data.BraTS_2020_subject_ID.to_list(),\n",
    "        data.Grade.to_numpy(),\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify=data.Grade.to_numpy(),\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"\"\"\n",
    "    Total Samples = {len(training)+len(validation)}\\n\n",
    "    Ratio of LGG:HGG in {len(training)} Training Samples:\n",
    "    \\t Ratio = {np.count_nonzero(train_check==0)/np.count_nonzero(train_check==1):.2f}\\n\n",
    "\n",
    "    Ratio of LGG:HGG in {len(validation)} Validation Samples:\n",
    "    \\t Ratio = {np.count_nonzero(val_check==0)/np.count_nonzero(val_check==1):.2f}\\n\n",
    "    \n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "    return (training, validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "def clear_gpu_cache():\n",
    "    \"\"\"Clear the PyTorch GPU Allocation if an OOM error occurs.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        print(\"Deleting Model\")\n",
    "        global model\n",
    "        del model\n",
    "    except NameError as e:\n",
    "        print(f\"Model Already Cleared\")\n",
    "\n",
    "    print(\"Collecting Garbage\")\n",
    "    gc.collect()\n",
    "    print(\"Clearing CUDA Cache\")\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Total Samples = 369\n",
      "\n",
      "    Ratio of LGG:HGG in 295 Training Samples:\n",
      "    \t Ratio = 0.26\n",
      "\n",
      "\n",
      "    Ratio of LGG:HGG in 74 Validation Samples:\n",
      "    \t Ratio = 0.25\n",
      "\n",
      "    \n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# Prepare list of All training Cases\n",
    "TRAINING_DATASET_PATH = r\"../MICCAI_BraTS2020_TrainingData/\"\n",
    "NAME_MAPPING = r\"../MICCAI_BraTS2020_TrainingData/name_mapping.csv\"\n",
    "\n",
    "# Function returns names of cases to be used\n",
    "train_cases, val_cases = train_val_test_dataset(NAME_MAPPING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare mapping to convert into PyTorch Dataset. Keeping T1 to make use of cached dataset\n",
    "train_cases = [\n",
    "    {\n",
    "        \"image\": [\n",
    "            path.join(TRAINING_DATASET_PATH, case, f\"{case}_t1.nii.gz\"),\n",
    "            path.join(TRAINING_DATASET_PATH, case, f\"{case}_t1ce.nii.gz\"),\n",
    "            path.join(TRAINING_DATASET_PATH, case, f\"{case}_t2.nii.gz\"),\n",
    "            path.join(TRAINING_DATASET_PATH, case, f\"{case}_flair.nii.gz\"),\n",
    "        ],\n",
    "        \"seg\": path.join(\n",
    "            TRAINING_DATASET_PATH, case, f\"{case}_seg.nii.gz\"\n",
    "        ),\n",
    "    }\n",
    "    for case in train_cases\n",
    "]\n",
    "\n",
    "val_cases = [\n",
    "    {\n",
    "        \"image\": [\n",
    "            path.join(TRAINING_DATASET_PATH, case, f\"{case}_t1.nii.gz\"),\n",
    "            path.join(TRAINING_DATASET_PATH, case, f\"{case}_t1ce.nii.gz\"),\n",
    "            path.join(TRAINING_DATASET_PATH, case, f\"{case}_t2.nii.gz\"),\n",
    "            path.join(TRAINING_DATASET_PATH, case, f\"{case}_flair.nii.gz\"),\n",
    "        ],\n",
    "        \"seg\": path.join(\n",
    "            TRAINING_DATASET_PATH, case, f\"{case}_seg.nii.gz\"\n",
    "        ),\n",
    "    }\n",
    "    for case in val_cases\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvertLabelsIntoOneHotd(MapTransform):\n",
    "    \"\"\"Takes input tensor of segmentation which contains\n",
    "    values in set (0,1,2,4) where\\n\n",
    "    0 -> Background/Normal\\n\n",
    "    1 -> Non- Enhancing Tumor Core\\n\n",
    "    2 -> Edema\\n\n",
    "    4 -> Enhancing tumor core\\n\n",
    "\n",
    "    and returns a one hot encoded 3 channel tensor where\n",
    "    1st Channel -> Whole tumor (1,2 and 4)\n",
    "    2nd Channel -> Tumor Core (1 and 4)\n",
    "    3rd Channel -> Enhancing Tumor core (4)\n",
    "    4th Channel ->\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, data):\n",
    "        data_dict = dict(data)\n",
    "        for key in self.keys:\n",
    "            one_hot_encode_array = [\n",
    "                torch.logical_or(\n",
    "                    torch.logical_or(data_dict[key] == 1, data_dict[key] == 2),\n",
    "                    data_dict[key] == 4,\n",
    "                ),  # Whole Tumor\n",
    "                torch.logical_or(data_dict[key] == 1, data_dict[key] == 4),  # Tumor Core\n",
    "                data_dict[key] == 4,  # Enhancing Core\n",
    "                \n",
    "            ]\n",
    "        data_dict[key] = torch.stack(one_hot_encode_array, axis=0).astype(torch.float32)\n",
    "        return data_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CalculateForeground(MapTransform):\n",
    "    \"\"\"Generate a Foreground tensor. Later append with image before passing into model\n",
    "    \"\"\"\n",
    "    def __call__(self, data):\n",
    "        d = dict(data)\n",
    "        for key in self.keys:\n",
    "\n",
    "            foreground_tensor = (\n",
    "                torch.logical_or(\n",
    "                    torch.logical_or(\n",
    "                        torch.logical_or(d[key][0, ...] > 0, d[key][1, ...] > 0),\n",
    "                        d[key][2, ...] > 0,\n",
    "                    ),\n",
    "                    d[key][3, ...] > 0,\n",
    "                ),\n",
    "            )\n",
    "\n",
    "\n",
    "            d['foreground'] = foreground_tensor[0].unsqueeze(0)\n",
    "            return d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform data such as from niftii into Tensors\n",
    "\n",
    "\n",
    "transform_validation_dataset = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"image\", \"seg\"]),\n",
    "        EnsureChannelFirstd(keys=[\"image\"]),\n",
    "        ConvertLabelsIntoOneHotd(keys=\"seg\"),\n",
    "        ToMetaTensord([\"image\", \"seg\"]),\n",
    "        Orientationd(keys=[\"image\", \"seg\"], axcodes=\"RAS\"),\n",
    "        NormalizeIntensityd(keys=\"image\", nonzero=True, channel_wise=True),\n",
    "    ]\n",
    ")\n",
    "# transform_validation_dataset = Compose(\n",
    "#     [\n",
    "#         LoadImaged(keys=[\"image\", \"seg\"]),\n",
    "#         EnsureChannelFirstd(keys=[\"image\"]),\n",
    "#         CalculateForeground(keys=[\"image\"]),\n",
    "#         ConvertLabelsIntoOneHotd(keys=\"seg\"),\n",
    "#         ToMetaTensord([\"image\", \"seg\", \"foreground\"]),\n",
    "#         Orientationd(keys=[\"image\", \"seg\", \"foreground\"], axcodes=\"RAS\"),\n",
    "#         NormalizeIntensityd(keys=\"image\", nonzero=True, channel_wise=True),\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "\n",
    "#! transformation in validation dataset\n",
    "val_data_loader = DataLoader(\n",
    "    PersistentDataset(\n",
    "        val_cases, transform_validation_dataset,\n",
    "        cache_dir='./cache_patch_experiment/'\n",
    "    ),\n",
    "    shuffle=False,\n",
    "    batch_size=1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logger \n",
    "\n",
    "logger = SummaryWriter(log_dir=\"./tensorboard_logs/Only 3 Channels\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trial(patchsize: tuple = (96, 96, 96), epochs=20, continue_training=0):\n",
    "    \"\"\"Given a patch size do the experiment for set epochs\n",
    "    and record the metric in each case.\n",
    "\n",
    "    Args:\n",
    "        patchsize (tuple, optional): Input Slice. Defaults to (96,96,96).\n",
    "    \"\"\"\n",
    "\n",
    "    clear_gpu_cache()\n",
    "\n",
    "    device = torch.device(\"cuda:0\") if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    val_interval = 2\n",
    "\n",
    "    model = UNet(\n",
    "        spatial_dims=3,\n",
    "        in_channels=3,\n",
    "        out_channels=3,\n",
    "        strides=(2, 2, 2),\n",
    "        channels=[16,32,64,128],\n",
    "        num_res_units=2,\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = Novograd(model.parameters(), weight_decay=0.0001)\n",
    "\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    lr_scheduler = WarmupCosineSchedule(\n",
    "        optimizer,\n",
    "        warmup_steps=int(epochs / 10),\n",
    "        warmup_multiplier=1e-10,\n",
    "        t_total=epochs,\n",
    "    )\n",
    "\n",
    "    scheduler_reduce_on_plat = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode=\"max\", factor=0.1, patience=20, threshold=0.01\n",
    "    )\n",
    "\n",
    "    loss_function = DiceCELoss(sigmoid=True, squared_pred=True)\n",
    "    # loss_function = DiceFocalLoss(sigmoid=True, squared_pred=True)\n",
    "    # loss_function = GeneralizedDiceFocalLoss(sigmoid=True)\n",
    "\n",
    "    # try:\n",
    "    #     chk = torch.load(\"./Patch_Size_Experiment_Results/Low_Learning_Rate/Best_Metric_PatchSize_(120, 120, 120).pth\")\n",
    "    #     model.load_state_dict(chk['model'])\n",
    "    #     optimizer.load_state_dict(chk['optimiser'])\n",
    "    # except Exception as e:\n",
    "    #     print(e)\n",
    "\n",
    "    # ? Check inferer parameters and tweak\n",
    "    def inference(input):\n",
    "        \"\"\"Do Sliding Window Inference on input tensor\n",
    "        To avoid OOM Error, Input Model done on CPU.\n",
    "        Patch taken from input and its inference done on GPU\n",
    "        to speed up inference time.\n",
    "\n",
    "        Args:\n",
    "            input: Full input to pass in the model. For the case\n",
    "            of this project size => (3,240,240,155)\n",
    "        \"\"\"\n",
    "\n",
    "        def _compute(input):\n",
    "            return sliding_window_inference(\n",
    "                inputs=input.to(\"cpu\"),\n",
    "                roi_size=patchsize,\n",
    "                sw_batch_size=1,\n",
    "                predictor=model,\n",
    "                overlap=0.5,\n",
    "                padding_mode=\"constant\",\n",
    "                sw_device=\"cuda:0\",\n",
    "                device=\"cpu\",\n",
    "                mode=\"constant\",\n",
    "            )\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            return _compute(input)\n",
    "\n",
    "    post_processing_validation = Compose(\n",
    "        [Activations(sigmoid=True), AsDiscrete(threshold=0.5)]\n",
    "    )\n",
    "\n",
    "    dice_metric = DiceMetric(include_background=True, reduction=\"mean\")\n",
    "    dice_metric_batch = DiceMetric(include_background=True, reduction=\"mean_batch\")\n",
    "\n",
    "    \n",
    "    transform_training_dataset = Compose(\n",
    "        [\n",
    "            LoadImaged(keys=[\"image\", \"seg\"]),\n",
    "            EnsureChannelFirstd(keys=[\"image\"]),\n",
    "            ConvertLabelsIntoOneHotd(keys=\"seg\"),\n",
    "            ToMetaTensord([\"image\", \"seg\"]),\n",
    "            Orientationd(keys=[\"image\", \"seg\"], axcodes=\"RAS\"),\n",
    "            NormalizeIntensityd(keys=\"image\", nonzero=True, channel_wise=True),\n",
    "            RandCropByPosNegLabeld(\n",
    "                keys=[\"image\", \"seg\"],\n",
    "                spatial_size=(128, 128, 128),\n",
    "                label_key=\"seg\",\n",
    "                neg=0,\n",
    "                num_samples=2,\n",
    "            ),\n",
    "            RandFlipd(keys=[\"image\", \"seg\"], prob=0.5, spatial_axis=0),\n",
    "            RandFlipd(keys=[\"image\", \"seg\"], prob=0.5, spatial_axis=1),\n",
    "            RandFlipd(keys=[\"image\", \"seg\"], prob=0.5, spatial_axis=2),\n",
    "            RandScaleIntensityd(keys=\"image\", factors=0.1, prob=1.0),\n",
    "            RandShiftIntensityd(keys=\"image\", offsets=0.1, prob=1.0),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    train_data_loader = DataLoader(\n",
    "        PersistentDataset(\n",
    "            train_cases,\n",
    "            transform_training_dataset,\n",
    "            cache_dir='./cache_patch_experiment/'\n",
    "        ),\n",
    "        shuffle=True,\n",
    "        batch_size=2,\n",
    "    )\n",
    "\n",
    "    best_metric = -1\n",
    "\n",
    "    for epoch in range(continue_training, epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        step = 0\n",
    "\n",
    "        logger.add_scalar(\"Learning_Rate\", optimizer.param_groups[0][\"lr\"], epoch)\n",
    "\n",
    "        for batch_data in tqdm(train_data_loader):\n",
    "\n",
    "            inputs, labels = (\n",
    "                batch_data[\"image\"][:,1:4,...].to(device),\n",
    "                # torch.concat(\n",
    "                #     [\n",
    "                #         batch_data[\"image\"].to(device),\n",
    "                #         batch_data[\"foreground\"].to(device),\n",
    "                #     ],\n",
    "                #     dim=1,\n",
    "                # ),\n",
    "                batch_data[\"seg\"].to(device),\n",
    "            )\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(inputs)\n",
    "                loss = loss_function(outputs, labels)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            step += 1\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        epoch_loss_value = epoch_loss / step\n",
    "\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        logger.add_scalar(\"Training/Loss\", epoch_loss_value, epoch)\n",
    "\n",
    "        if (epoch + 1) % val_interval == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for val_data in tqdm(val_data_loader):\n",
    "                    val_inputs, val_labels = (\n",
    "                        val_data[\"image\"][:,1:4,...],\n",
    "                        # torch.concat(\n",
    "                        #     [val_data[\"image\"], val_data[\"foreground\"]], dim=1\n",
    "                        # ),\n",
    "                        val_data[\"seg\"].to(device),\n",
    "                    )\n",
    "\n",
    "                    val_outputs = inference(val_inputs)\n",
    "\n",
    "                    val_outputs = [\n",
    "                        post_processing_validation(i)\n",
    "                        for i in decollate_batch(val_outputs.to(device))\n",
    "                    ]\n",
    "                    dice_metric(y_pred=val_outputs, y=val_labels)\n",
    "                    dice_metric_batch(y_pred=val_outputs, y=val_labels)\n",
    "\n",
    "                metric = dice_metric.aggregate().item()\n",
    "\n",
    "                scheduler_reduce_on_plat.step(metric)\n",
    "\n",
    "                metric_batch = dice_metric_batch.aggregate()\n",
    "                metric_wt = metric_batch[0].item()\n",
    "                metric_tc = metric_batch[1].item()\n",
    "                metric_et = metric_batch[2].item()\n",
    "\n",
    "                if metric > best_metric:\n",
    "                    best_metric = metric\n",
    "                    torch.save(\n",
    "                        {\n",
    "                            \"model\": model.state_dict(),\n",
    "                            \"optimiser\": optimizer.state_dict(),\n",
    "                            \"scheduler\": lr_scheduler.state_dict(),\n",
    "                            \"best_metric_epoch\": epoch,\n",
    "                            \"best_metric\": best_metric\n",
    "                        },\n",
    "                        f\"./Patch_Size_Experiment_Results/best_metric_3_channels_wot1.pth\",\n",
    "                    )\n",
    "\n",
    "                logger.add_scalar(\"DICE/Average\", metric, epoch)\n",
    "                logger.add_scalar(\"DICE/WT\", metric_wt, epoch)\n",
    "                logger.add_scalar(\"DICE/TC\", metric_tc, epoch)\n",
    "                logger.add_scalar(\"DICE/ET\", metric_et, epoch)\n",
    "\n",
    "                dice_metric.reset()\n",
    "                dice_metric_batch.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting Model\n",
      "Model Already Cleared\n",
      "Collecting Garbage\n",
      "Clearing CUDA Cache\n",
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 148/148 [07:03<00:00,  2.86s/it]\n",
      "100%|██████████| 148/148 [07:04<00:00,  2.87s/it]\n",
      "100%|██████████| 74/74 [03:31<00:00,  2.86s/it]\n",
      "100%|██████████| 148/148 [07:04<00:00,  2.87s/it]\n",
      "100%|██████████| 148/148 [07:04<00:00,  2.87s/it]\n",
      "100%|██████████| 74/74 [03:32<00:00,  2.87s/it]\n",
      "100%|██████████| 148/148 [07:04<00:00,  2.87s/it]\n",
      "100%|██████████| 148/148 [07:03<00:00,  2.86s/it]\n",
      "100%|██████████| 74/74 [03:32<00:00,  2.87s/it]\n",
      "100%|██████████| 148/148 [07:05<00:00,  2.87s/it]\n",
      "100%|██████████| 148/148 [07:04<00:00,  2.87s/it]\n",
      "100%|██████████| 74/74 [03:32<00:00,  2.87s/it]\n",
      "100%|██████████| 148/148 [07:04<00:00,  2.87s/it]\n",
      "100%|██████████| 148/148 [07:04<00:00,  2.87s/it]\n",
      "100%|██████████| 74/74 [03:32<00:00,  2.87s/it]\n",
      "100%|██████████| 148/148 [07:05<00:00,  2.87s/it]\n",
      "100%|██████████| 148/148 [07:04<00:00,  2.87s/it]\n",
      "100%|██████████| 74/74 [03:32<00:00,  2.86s/it]\n",
      "100%|██████████| 148/148 [07:04<00:00,  2.87s/it]\n",
      "100%|██████████| 148/148 [07:02<00:00,  2.85s/it]\n",
      "100%|██████████| 74/74 [03:31<00:00,  2.86s/it]\n",
      "100%|██████████| 148/148 [07:03<00:00,  2.86s/it]\n",
      "100%|██████████| 148/148 [07:01<00:00,  2.85s/it]\n",
      "100%|██████████| 74/74 [03:31<00:00,  2.85s/it]\n",
      "100%|██████████| 148/148 [07:03<00:00,  2.86s/it]\n",
      "100%|██████████| 148/148 [07:03<00:00,  2.86s/it]\n",
      "100%|██████████| 74/74 [03:31<00:00,  2.86s/it]\n",
      "100%|██████████| 148/148 [07:03<00:00,  2.86s/it]\n",
      "100%|██████████| 148/148 [07:01<00:00,  2.85s/it]\n",
      "100%|██████████| 74/74 [03:31<00:00,  2.85s/it]\n",
      "100%|██████████| 148/148 [07:03<00:00,  2.86s/it]\n",
      "100%|██████████| 148/148 [07:02<00:00,  2.86s/it]\n",
      "100%|██████████| 74/74 [03:31<00:00,  2.86s/it]\n",
      "100%|██████████| 148/148 [07:03<00:00,  2.86s/it]\n",
      "100%|██████████| 148/148 [07:01<00:00,  2.85s/it]\n",
      "100%|██████████| 74/74 [03:31<00:00,  2.86s/it]\n",
      "100%|██████████| 148/148 [07:03<00:00,  2.86s/it]\n",
      "100%|██████████| 148/148 [07:03<00:00,  2.86s/it]\n",
      "100%|██████████| 74/74 [03:31<00:00,  2.86s/it]\n",
      "100%|██████████| 148/148 [07:03<00:00,  2.86s/it]\n",
      "100%|██████████| 148/148 [07:02<00:00,  2.85s/it]\n",
      "100%|██████████| 74/74 [03:31<00:00,  2.86s/it]\n",
      "100%|██████████| 148/148 [07:03<00:00,  2.86s/it]\n",
      "100%|██████████| 148/148 [07:02<00:00,  2.86s/it]\n",
      "100%|██████████| 74/74 [03:31<00:00,  2.86s/it]\n",
      "100%|██████████| 148/148 [07:03<00:00,  2.86s/it]\n",
      "100%|██████████| 148/148 [07:02<00:00,  2.85s/it]\n",
      "100%|██████████| 74/74 [03:31<00:00,  2.86s/it]\n",
      "100%|██████████| 148/148 [07:03<00:00,  2.86s/it]\n",
      "100%|██████████| 148/148 [07:01<00:00,  2.85s/it]\n",
      "100%|██████████| 74/74 [03:31<00:00,  2.86s/it]\n",
      "100%|██████████| 148/148 [07:03<00:00,  2.86s/it]\n",
      "100%|██████████| 148/148 [07:03<00:00,  2.86s/it]\n",
      "100%|██████████| 74/74 [03:31<00:00,  2.86s/it]\n",
      "100%|██████████| 148/148 [07:03<00:00,  2.86s/it]\n",
      "100%|██████████| 148/148 [07:01<00:00,  2.85s/it]\n",
      "100%|██████████| 74/74 [03:31<00:00,  2.86s/it]\n",
      "100%|██████████| 148/148 [07:03<00:00,  2.86s/it]\n",
      "100%|██████████| 148/148 [07:01<00:00,  2.85s/it]\n",
      "100%|██████████| 74/74 [03:31<00:00,  2.86s/it]\n",
      "100%|██████████| 148/148 [07:03<00:00,  2.86s/it]\n",
      "100%|██████████| 148/148 [07:03<00:00,  2.86s/it]\n",
      "100%|██████████| 74/74 [03:31<00:00,  2.86s/it]\n",
      "100%|██████████| 148/148 [07:03<00:00,  2.86s/it]\n",
      "100%|██████████| 148/148 [07:03<00:00,  2.86s/it]\n",
      "100%|██████████| 74/74 [03:31<00:00,  2.86s/it]\n",
      "100%|██████████| 148/148 [07:03<00:00,  2.86s/it]\n",
      "100%|██████████| 148/148 [07:02<00:00,  2.86s/it]\n",
      "100%|██████████| 74/74 [03:31<00:00,  2.86s/it]\n",
      "100%|██████████| 148/148 [07:03<00:00,  2.86s/it]\n",
      "100%|██████████| 148/148 [07:03<00:00,  2.86s/it]\n",
      "100%|██████████| 74/74 [03:31<00:00,  2.86s/it]\n",
      "100%|██████████| 148/148 [07:04<00:00,  2.87s/it]\n",
      "100%|██████████| 148/148 [07:03<00:00,  2.86s/it]\n",
      "100%|██████████| 74/74 [03:31<00:00,  2.86s/it]\n",
      "100%|██████████| 148/148 [07:05<00:00,  2.88s/it]\n",
      "100%|██████████| 148/148 [07:02<00:00,  2.86s/it]\n",
      "100%|██████████| 74/74 [03:32<00:00,  2.87s/it]\n",
      "100%|██████████| 148/148 [07:05<00:00,  2.88s/it]\n",
      "100%|██████████| 148/148 [07:03<00:00,  2.86s/it]\n",
      "100%|██████████| 74/74 [03:32<00:00,  2.87s/it]\n",
      "100%|██████████| 148/148 [07:04<00:00,  2.87s/it]\n",
      "100%|██████████| 148/148 [07:05<00:00,  2.88s/it]\n",
      "100%|██████████| 74/74 [03:32<00:00,  2.88s/it]\n",
      "100%|██████████| 148/148 [07:04<00:00,  2.87s/it]\n",
      "100%|██████████| 148/148 [07:03<00:00,  2.86s/it]\n",
      "100%|██████████| 74/74 [03:32<00:00,  2.87s/it]\n",
      "100%|██████████| 148/148 [07:05<00:00,  2.87s/it]\n",
      "100%|██████████| 148/148 [07:08<00:00,  2.89s/it]\n",
      "100%|██████████| 74/74 [03:35<00:00,  2.91s/it]\n",
      "100%|██████████| 148/148 [07:05<00:00,  2.87s/it]\n",
      "100%|██████████| 148/148 [07:04<00:00,  2.87s/it]\n",
      "100%|██████████| 74/74 [03:32<00:00,  2.87s/it]\n",
      "100%|██████████| 148/148 [07:05<00:00,  2.87s/it]\n",
      "100%|██████████| 148/148 [07:05<00:00,  2.88s/it]\n",
      "100%|██████████| 74/74 [03:35<00:00,  2.92s/it]\n",
      "100%|██████████| 148/148 [07:07<00:00,  2.89s/it]\n",
      "100%|██████████| 148/148 [07:02<00:00,  2.86s/it]\n",
      "100%|██████████| 74/74 [03:32<00:00,  2.87s/it]\n",
      "100%|██████████| 148/148 [07:04<00:00,  2.87s/it]\n",
      "100%|██████████| 148/148 [07:03<00:00,  2.86s/it]\n",
      "100%|██████████| 74/74 [03:34<00:00,  2.90s/it]\n",
      "100%|██████████| 148/148 [07:08<00:00,  2.90s/it]\n",
      "100%|██████████| 148/148 [07:05<00:00,  2.87s/it]\n",
      "100%|██████████| 74/74 [03:35<00:00,  2.91s/it]\n",
      " 47%|████▋     | 70/148 [03:23<03:46,  2.90s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39mfor\u001b[39;00m patchsize \u001b[39min\u001b[39;00m patchsizes:\n\u001b[0;32m     10\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 11\u001b[0m         trial(patchsize,epochs\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m)\n\u001b[0;32m     12\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     13\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mException for patch size \u001b[39m\u001b[39m{\u001b[39;00mpatchsize\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[9], line 128\u001b[0m, in \u001b[0;36mtrial\u001b[1;34m(patchsize, epochs, continue_training)\u001b[0m\n\u001b[0;32m    124\u001b[0m step \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m    126\u001b[0m logger\u001b[39m.\u001b[39madd_scalar(\u001b[39m\"\u001b[39m\u001b[39mLearning_Rate\u001b[39m\u001b[39m\"\u001b[39m, optimizer\u001b[39m.\u001b[39mparam_groups[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m\"\u001b[39m], epoch)\n\u001b[1;32m--> 128\u001b[0m \u001b[39mfor\u001b[39;00m batch_data \u001b[39min\u001b[39;00m tqdm(train_data_loader):\n\u001b[0;32m    130\u001b[0m     inputs, labels \u001b[39m=\u001b[39m (\n\u001b[0;32m    131\u001b[0m         batch_data[\u001b[39m\"\u001b[39m\u001b[39mimage\u001b[39m\u001b[39m\"\u001b[39m][:,\u001b[39m1\u001b[39m:\u001b[39m4\u001b[39m,\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m]\u001b[39m.\u001b[39mto(device),\n\u001b[0;32m    132\u001b[0m         \u001b[39m# torch.concat(\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    139\u001b[0m         batch_data[\u001b[39m\"\u001b[39m\u001b[39mseg\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mto(device),\n\u001b[0;32m    140\u001b[0m     )\n\u001b[0;32m    142\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\ibrah\\miniconda3\\envs\\fyp\\lib\\site-packages\\tqdm\\std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1192\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[0;32m   1194\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1195\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[0;32m   1196\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[0;32m   1197\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1198\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ibrah\\miniconda3\\envs\\fyp\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\ibrah\\miniconda3\\envs\\fyp\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    669\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    670\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 671\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    672\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    673\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\ibrah\\miniconda3\\envs\\fyp\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\ibrah\\miniconda3\\envs\\fyp\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\ibrah\\miniconda3\\envs\\fyp\\lib\\site-packages\\monai\\data\\dataset.py:107\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(index, collections\u001b[39m.\u001b[39mabc\u001b[39m.\u001b[39mSequence):\n\u001b[0;32m    105\u001b[0m     \u001b[39m# dataset[[1, 3, 4]]\u001b[39;00m\n\u001b[0;32m    106\u001b[0m     \u001b[39mreturn\u001b[39;00m Subset(dataset\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m, indices\u001b[39m=\u001b[39mindex)\n\u001b[1;32m--> 107\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_transform(index)\n",
      "File \u001b[1;32mc:\\Users\\ibrah\\miniconda3\\envs\\fyp\\lib\\site-packages\\monai\\data\\dataset.py:410\u001b[0m, in \u001b[0;36mPersistentDataset._transform\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    409\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_transform\u001b[39m(\u001b[39mself\u001b[39m, index: \u001b[39mint\u001b[39m):\n\u001b[1;32m--> 410\u001b[0m     pre_random_item \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cachecheck(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata[index])\n\u001b[0;32m    411\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_post_transform(pre_random_item)\n",
      "File \u001b[1;32mc:\\Users\\ibrah\\miniconda3\\envs\\fyp\\lib\\site-packages\\monai\\data\\dataset.py:378\u001b[0m, in \u001b[0;36mPersistentDataset._cachecheck\u001b[1;34m(self, item_transformed)\u001b[0m\n\u001b[0;32m    376\u001b[0m \u001b[39mif\u001b[39;00m hashfile \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m hashfile\u001b[39m.\u001b[39mis_file():  \u001b[39m# cache hit\u001b[39;00m\n\u001b[0;32m    377\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 378\u001b[0m         \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mload(hashfile)\n\u001b[0;32m    379\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mPermissionError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    380\u001b[0m         \u001b[39mif\u001b[39;00m sys\u001b[39m.\u001b[39mplatform \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mwin32\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\ibrah\\miniconda3\\envs\\fyp\\lib\\site-packages\\torch\\serialization.py:789\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[0;32m    787\u001b[0m             \u001b[39mexcept\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    788\u001b[0m                 \u001b[39mraise\u001b[39;00m pickle\u001b[39m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(e)) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m--> 789\u001b[0m         \u001b[39mreturn\u001b[39;00m _load(opened_zipfile, map_location, pickle_module, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[0;32m    790\u001b[0m \u001b[39mif\u001b[39;00m weights_only:\n\u001b[0;32m    791\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\ibrah\\miniconda3\\envs\\fyp\\lib\\site-packages\\torch\\serialization.py:1131\u001b[0m, in \u001b[0;36m_load\u001b[1;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1129\u001b[0m unpickler \u001b[39m=\u001b[39m UnpicklerWrapper(data_file, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[0;32m   1130\u001b[0m unpickler\u001b[39m.\u001b[39mpersistent_load \u001b[39m=\u001b[39m persistent_load\n\u001b[1;32m-> 1131\u001b[0m result \u001b[39m=\u001b[39m unpickler\u001b[39m.\u001b[39;49mload()\n\u001b[0;32m   1133\u001b[0m torch\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[0;32m   1135\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\ibrah\\miniconda3\\envs\\fyp\\lib\\site-packages\\torch\\serialization.py:1101\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[1;34m(saved_id)\u001b[0m\n\u001b[0;32m   1099\u001b[0m \u001b[39mif\u001b[39;00m key \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m loaded_storages:\n\u001b[0;32m   1100\u001b[0m     nbytes \u001b[39m=\u001b[39m numel \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_element_size(dtype)\n\u001b[1;32m-> 1101\u001b[0m     load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))\n\u001b[0;32m   1103\u001b[0m \u001b[39mreturn\u001b[39;00m loaded_storages[key]\n",
      "File \u001b[1;32mc:\\Users\\ibrah\\miniconda3\\envs\\fyp\\lib\\site-packages\\torch\\serialization.py:1079\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[1;34m(dtype, numel, key, location)\u001b[0m\n\u001b[0;32m   1076\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_tensor\u001b[39m(dtype, numel, key, location):\n\u001b[0;32m   1077\u001b[0m     name \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mdata/\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[1;32m-> 1079\u001b[0m     storage \u001b[39m=\u001b[39m zip_file\u001b[39m.\u001b[39;49mget_storage_from_record(name, numel, torch\u001b[39m.\u001b[39;49mUntypedStorage)\u001b[39m.\u001b[39mstorage()\u001b[39m.\u001b[39muntyped()\n\u001b[0;32m   1080\u001b[0m     \u001b[39m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[0;32m   1081\u001b[0m     \u001b[39m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[0;32m   1082\u001b[0m     loaded_storages[key] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstorage\u001b[39m.\u001b[39mTypedStorage(\n\u001b[0;32m   1083\u001b[0m         wrap_storage\u001b[39m=\u001b[39mrestore_location(storage, location),\n\u001b[0;32m   1084\u001b[0m         dtype\u001b[39m=\u001b[39mdtype)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "patchsizes = [\n",
    "    # (96,96,96),\n",
    "    # (80,80,80),\n",
    "    (128,128,128),\n",
    "    # (160,160,160),\n",
    "    # (200,200,200)\n",
    "]\n",
    "\n",
    "for patchsize in patchsizes:\n",
    "    try:\n",
    "        trial(patchsize,epochs=100)\n",
    "    except Exception as e:\n",
    "        print(f\"Exception for patch size {patchsize}\")\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fyp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "37bd13aa3d924eb96db98b00aa626978b2fce1020e2653180f4604a647daf9d4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
