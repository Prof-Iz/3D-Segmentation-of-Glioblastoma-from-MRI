{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Just Experimenting: Skip Section to Next\n",
    "\n",
    "This section is just exploration and getting my bearings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from monai.transforms import LoadImaged, MapTransform, Compose, ToNumpyd\n",
    "from monai.data import Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "Validation_Cases = list(glob.glob(\"./MICCAI_BraTS2020_TrainingData/BraTS20*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_cases = []\n",
    "\n",
    "for case in Validation_Cases:\n",
    "    try:\n",
    "        t1 = list(glob.glob(f\"{case}/*t1.nii.gz\"))\n",
    "        t1ce = list(glob.glob(f\"{case}/*t1ce.nii.gz\"))\n",
    "        t2 = list(glob.glob(f\"{case}/*t2.nii.gz\"))\n",
    "        flair = list(glob.glob(f\"{case}/*flair.nii.gz\"))\n",
    "        seg = glob.glob(f\"{case}/*seg.nii.gz\")[0]\n",
    "\n",
    "        img = t1 + t1ce + t2 + flair\n",
    "\n",
    "        d = {\"image\": img, \"seg\": seg}\n",
    "        experiment_cases.append(d)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(case)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvertLabelsIntoOneHotd(MapTransform):\n",
    "    \"\"\"Takes input tensor of segmentation which contains\n",
    "    values in set (0,1,2,4) where\\n\n",
    "    0 -> Background/Normal\\n\n",
    "    1 -> Non- Enhancing Tumor Core\\n\n",
    "    2 -> Edema\\n\n",
    "    4 -> Enhancing tumor core\\n\n",
    "\n",
    "    and returns a one hot encoded 3 channel tensor where\n",
    "    1st Channel -> Whole tumor (1,2 and 4)\n",
    "    2nd Channel -> Tumor Core (1 and 4)\n",
    "    3rd Channel -> Enhancing Tumor core (4)\n",
    "    \"\"\"\n",
    "    def __call__(self, data):\n",
    "        data_dict = dict(data)\n",
    "        for key in self.keys:\n",
    "            one_hot_encode_array = [\n",
    "                torch.logical_or(\n",
    "                    torch.logical_or(data_dict[key] == 1, data_dict[key] == 2),\n",
    "                    data_dict[key] == 4,\n",
    "                ), # Whole Tumor\n",
    "                torch.logical_or(data_dict[key] == 1, data_dict[key] == 4), # Tumor Core\n",
    "                data_dict[key] == 4, # Enhancing Core\n",
    "            ]\n",
    "            data_dict[key] = torch.stack(one_hot_encode_array, axis=0).astype(torch.int32)\n",
    "        return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_for_analysis = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"image\", \"seg\"]),\n",
    "        ConvertLabelsIntoOneHotd(keys=\"seg\"),\n",
    "        ToNumpyd(keys=[\"image\", \"seg\"])\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(experiment_cases,transform_for_analysis)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Textural Analysis\n",
    "\n",
    "1. Take GLCM from Tumor Boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to find boundary from segmentation\n",
    "from scipy import ndimage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "case = 25\n",
    "wt = dataset[case]['seg'][0]\n",
    "tc = dataset[case]['seg'][1]\n",
    "et = dataset[case]['seg'][2]\n",
    "\n",
    "t1 = dataset[case]['image'][0]\n",
    "t1ce = dataset[case]['image'][1]\n",
    "t2 = dataset[case]['image'][2]\n",
    "flair = dataset[case]['image'][3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 WT blob found\n",
      "1 TC blob found\n",
      "1 ET blob found\n"
     ]
    }
   ],
   "source": [
    "blob_wt = ndimage.find_objects(wt)\n",
    "print(f\"{len(blob_wt)} WT blob found\")\n",
    "blob_tc = ndimage.find_objects(tc)\n",
    "print(f\"{len(blob_tc)} TC blob found\")\n",
    "blob_et = ndimage.find_objects(et)\n",
    "print(f\"{len(blob_et)} ET blob found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Whole Tumor\n",
      "Blob 0: coordinate (30.557043041114397, 53.65276439778981, 42.876817371469066) and 180.618 cm^3\n",
      "\n",
      "For Tumor Core\n",
      "Blob 0: coordinate (21.879299974221638, 32.3802308595652, 25.608541231060062) and 34.913 cm^3\n",
      "\n",
      "For Enhancing Tumor\n",
      "Blob 0: coordinate (23.28733738807231, 32.01900658895084, 27.14660415610745) and 23.676 cm^3\n"
     ]
    }
   ],
   "source": [
    "# Finding Center of Mass and Volume\n",
    "\n",
    "print(\"For Whole Tumor\")\n",
    "for i,blob in enumerate(blob_wt):\n",
    "    print(f\"Blob {i}: coordinate {ndimage.center_of_mass(wt[blob])} and {np.count_nonzero(wt[blob]) / 10**3} cm^3\")\n",
    "\n",
    "\n",
    "print(\"\\nFor Tumor Core\")\n",
    "for i,blob in enumerate(blob_tc):\n",
    "    print(f\"Blob {i}: coordinate {ndimage.center_of_mass(tc[blob])} and {np.count_nonzero(tc[blob]) / 10**3} cm^3\")\n",
    "\n",
    "\n",
    "print(\"\\nFor Enhancing Tumor\")\n",
    "for i,blob in enumerate(blob_et):\n",
    "    print(f\"Blob {i}: coordinate {ndimage.center_of_mass(et[blob])} and {np.count_nonzero(et[blob]) / 10**3} cm^3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLCM\n",
    "\n",
    "# Based on https://github.com/Prof-Iz/GLCM-from-3D-NumPy-Input\n",
    "\n",
    "def glcm_3d(input: np.ndarray, delta: tuple[int] = (1, 1, 1), d: int = 1):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        input (np.ndarray): input array. 3D. dtype int\n",
    "        delta (tuple[int], optional): Direction vector from pixel. Defaults to (1, 1, 1).\n",
    "        d (int, optional): Distance to check for neighbouring channel. Defaults to 1.\n",
    "\n",
    "    Raises:\n",
    "        Exception: if input is not of type dint or is not 3D\n",
    "\n",
    "    Returns:\n",
    "        _type_: GLCM Matrix\n",
    "    \"\"\"\n",
    "\n",
    "    if 'int' not in input.dtype.__str__():\n",
    "        raise Exception(\"Input should be of dtype Int\")\n",
    "\n",
    "    if len(input.shape) != 3:\n",
    "        raise Exception(\"Input should be 3 dimensional\")\n",
    "\n",
    "    offset = (delta[0] * d, delta[1] * d, delta[2] * d)  # offset from each pixel\n",
    "\n",
    "    x_max, y_max, z_max = input.shape  # boundary conditions during enumeration\n",
    "\n",
    "    levels = input.max() + 1 # 0:1:n assume contn range of pixel values\n",
    "\n",
    "    print(f\"levels : {levels}\")\n",
    "    results = np.zeros((levels, levels))  # initialise results error\n",
    "\n",
    "\n",
    "    for i, v in np.ndenumerate(input):\n",
    "        x_offset = i[0] + offset[0]\n",
    "        y_offset = i[1] + offset[1]\n",
    "        z_offset = i[2] + offset[2]\n",
    "\n",
    "        if (x_offset >= x_max) or (y_offset >= y_max) or (z_offset >= z_max):\n",
    "            # if offset out of boundary skip\n",
    "            continue\n",
    "\n",
    "        value_at_offset = input[x_offset, y_offset, z_offset]\n",
    "\n",
    "        results[v, value_at_offset] += 1\n",
    "\n",
    "    return results / levels**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(66, 110, 79)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find GLCM for a blob in WT\n",
    "\n",
    "wt_tumor_boundary_t1    = t1[blob_wt[0]]\n",
    "wt_tumor_boundary_t1ce  = t1ce[blob_wt[0]]\n",
    "wt_tumor_boundary_t2    = t2[blob_wt[0]]\n",
    "wt_tumor_boundary_flair = flair[blob_wt[0]]\n",
    "\n",
    "wt_tumor_boundary_t1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'energy': 0.010458012361037416, 'homogeneity': 0.14261134493343397, 'contrast': 7161.607885927293, 'entropy': 10.093394917491178, 'correlation': 0.9076729367777118}\n"
     ]
    }
   ],
   "source": [
    "print(graycoprops(glcm))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Functions for each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import ndimage\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLCM\n",
    "\n",
    "# Based on https://github.com/Prof-Iz/GLCM-from-3D-NumPy-Input\n",
    "\n",
    "\n",
    "def glcm_3d(input: np.ndarray, delta: tuple[int] = (1, 1, 1), d: int = 1):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        input (np.ndarray): input array. 3D. dtype int\n",
    "        delta (tuple[int], optional): Direction vector from pixel. Defaults to (1, 1, 1).\n",
    "        d (int, optional): Distance to check for neighbouring channel. Defaults to 1.\n",
    "\n",
    "    Raises:\n",
    "        Exception: if input is not of type dint or is not 3D\n",
    "\n",
    "    Returns:\n",
    "        _type_: GLCM Matrix\n",
    "    \"\"\"\n",
    "\n",
    "    if \"int\" not in input.dtype.__str__():\n",
    "        raise Exception(\"Input should be of dtype Int\")\n",
    "\n",
    "    if len(input.shape) != 3:\n",
    "        raise Exception(\"Input should be 3 dimensional\")\n",
    "\n",
    "    offset = (delta[0] * d, delta[1] * d, delta[2] * d)  # offset from each pixel\n",
    "\n",
    "    x_max, y_max, z_max = input.shape  # boundary conditions during enumeration\n",
    "\n",
    "    levels = input.max() + 1  # 0:1:n assume contn range of pixel values\n",
    "\n",
    "    results = np.zeros((levels, levels))  # initialise results error\n",
    "\n",
    "    for i, v in np.ndenumerate(input):\n",
    "        x_offset = i[0] + offset[0]\n",
    "        y_offset = i[1] + offset[1]\n",
    "        z_offset = i[2] + offset[2]\n",
    "\n",
    "        if (x_offset >= x_max) or (y_offset >= y_max) or (z_offset >= z_max):\n",
    "            # if offset out of boundary skip\n",
    "            continue\n",
    "\n",
    "        value_at_offset = input[x_offset, y_offset, z_offset]\n",
    "\n",
    "        results[v, value_at_offset] += 1\n",
    "\n",
    "    return results / levels**2\n",
    "\n",
    "\n",
    "def normalise(P):\n",
    "    return P / np.sum(P)\n",
    "\n",
    "\n",
    "def graycoprops(P, debug=False):\n",
    "\n",
    "    P = normalise(P)\n",
    "\n",
    "    energy = np.sum(P**2)\n",
    "\n",
    "    idx = np.where(P > 0)\n",
    "\n",
    "    # begin i , j indexing from 1\n",
    "    i = idx[0] + 1\n",
    "    j = idx[1] + 1\n",
    "\n",
    "    homogeneity = np.sum(P[idx] / (1 + (i - j) ** 2))\n",
    "\n",
    "    contrast = np.sum(P[idx] * (i - j) ** 2)\n",
    "\n",
    "    entropy = np.sum(-P[idx] * np.log(P[idx]))\n",
    "\n",
    "    # for correlation\n",
    "\n",
    "    mu = np.sum(i * P[idx])\n",
    "\n",
    "    sigma_square = np.sum(P[idx] * (i - mu) ** 2)\n",
    "\n",
    "    correlation = np.sum((P[idx] * (i - mu) * (j - mu)) / sigma_square)\n",
    "\n",
    "    if debug:\n",
    "        print(\n",
    "            f\"\"\"\n",
    "        i: {i}\\n\n",
    "        j: {j}\\n\n",
    "        mu: {mu}\\n\n",
    "        sigma_square: {sigma_square}\\n\n",
    "        \"\"\"\n",
    "        )\n",
    "\n",
    "    return {\n",
    "        \"energy\": energy,\n",
    "        \"homogeneity\": homogeneity,\n",
    "        \"contrast\": contrast,\n",
    "        \"entropy\": entropy,\n",
    "        \"correlation\": correlation,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(input):\n",
    "    seg = np.squeeze(np.uint32(input[\"seg\"]))\n",
    "    scan = np.squeeze(np.uint32(input[\"image\"]))\n",
    "\n",
    "    wt = seg[0]\n",
    "    tc = seg[1]\n",
    "    et = seg[2]\n",
    "\n",
    "    t1 = scan[0]\n",
    "    t1ce = scan[1]\n",
    "    t2 = scan[2]\n",
    "    flair = scan[3]\n",
    "\n",
    "    feature = {}\n",
    "    blob_wt = ndimage.find_objects(wt)\n",
    "    blob_tc = ndimage.find_objects(tc)\n",
    "    blob_et = ndimage.find_objects(et)\n",
    "\n",
    "    \n",
    "    feature['blobs'] = {\n",
    "        \"blob_wt\" : len(blob_wt),\n",
    "        \"blob_tc\" : len(blob_tc),\n",
    "        \"blob_et\" : len(blob_et),\n",
    "\n",
    "    }\n",
    "\n",
    "\n",
    "    # assume 0th index of blob is largest one\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    glcm_t1 = glcm_3d(t1[blob_wt[0]])\n",
    "    glcm_t1ce = glcm_3d(t1ce[blob_wt[0]])\n",
    "    glcm_t2 = glcm_3d(t2[blob_wt[0]])\n",
    "    glcm_flair = glcm_3d(flair[blob_wt[0]])\n",
    "\n",
    "    # Textural analysis on Largest WT blob\n",
    "    feature['textural'] = {\n",
    "        't1' : graycoprops(glcm_t1),\n",
    "        't1ce' : graycoprops(glcm_t1ce),\n",
    "        't2' : graycoprops(glcm_t2),\n",
    "        'flair' : graycoprops(glcm_flair)\n",
    "    }\n",
    "\n",
    "\n",
    "    \n",
    "    wt_non_txt = {\n",
    "            \"center\": ndimage.center_of_mass(wt[blob_wt[0]]),  # center of mass\n",
    "            \"volume\": np.count_nonzero(wt[blob_wt[0]]) / 10**3,  # volume in cm^3\n",
    "        }\n",
    "    tc_non_txt = {\n",
    "            \"center\": ndimage.center_of_mass(tc[blob_tc[0]]),  # center of mass\n",
    "            \"volume\": np.count_nonzero(tc[blob_tc[0]]) / 10**3,  # volume in cm^3\n",
    "        }\n",
    "    et_non_txt = {\n",
    "            \"center\": ndimage.center_of_mass(et[blob_et[0]]),  # center of mass\n",
    "            \"volume\": np.count_nonzero(et[blob_et[0]]) / 10**3,  # volume in cm^3\n",
    "        }\n",
    "\n",
    "    feature['non-textural'] = {\n",
    "        \"wt\" : wt_non_txt,\n",
    "        \"tc\" : tc_non_txt,\n",
    "        \"et\" : et_non_txt\n",
    "    }\n",
    "\n",
    "    return feature\n",
    "\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating for Ground Truth Vs. Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.transforms import (\n",
    "    Activations,\n",
    "    AsDiscrete,\n",
    "    LoadImaged,\n",
    "    Compose,\n",
    "    MapTransform,\n",
    "    EnsureChannelFirstd,\n",
    "    ToMetaTensord,\n",
    "    Orientationd,\n",
    "    NormalizeIntensityd,\n",
    ")\n",
    "from monai.networks.nets import UNet\n",
    "from monai.metrics import DiceMetric, HausdorffDistanceMetric\n",
    "import glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = UNet(\n",
    "    spatial_dims=3,\n",
    "    in_channels=4,\n",
    "    out_channels=3,\n",
    "    strides=(2, 2, 2),\n",
    "    channels=[16,32,64,128],\n",
    "    num_res_units=2,\n",
    ").to(device)\n",
    "\n",
    "chk = torch.load(\"./experiments/Patch_Size_Experiment_Results/best_metric_dyn_unet_dicece_2res_chan128div3_const_sliding.pth\")\n",
    "\n",
    "\n",
    "model.load_state_dict(chk[\"model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "Validation_Cases = list(glob.glob(\"./MICCAI_BraTS2020_TrainingData/*\"))\n",
    "\n",
    "experiment_cases = []\n",
    "\n",
    "for case in Validation_Cases:\n",
    "    t1 = list(glob.glob(f\"{case}/*t1.nii.gz\"))\n",
    "    t1ce = list(glob.glob(f\"{case}/*t1ce.nii.gz\"))\n",
    "    t2 = list(glob.glob(f\"{case}/*t2.nii.gz\"))\n",
    "    flair = list(glob.glob(f\"{case}/*flair.nii.gz\"))\n",
    "    seg = glob.glob(f\"{case}/*seg.nii.gz\")[0]\n",
    "\n",
    "    img = t1 + t1ce + t2 + flair\n",
    "\n",
    "    d = {\"image\": img, \"seg\": seg}\n",
    "    experiment_cases.append(d)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvertLabelsIntoOneHotd(MapTransform):\n",
    "    \"\"\"Takes input tensor of segmentation which contains\n",
    "    values in set (0,1,2,4) where\\n\n",
    "    0 -> Background/Normal\\n\n",
    "    1 -> Non- Enhancing Tumor Core\\n\n",
    "    2 -> Edema\\n\n",
    "    4 -> Enhancing tumor core\\n\n",
    "\n",
    "    and returns a one hot encoded 3 channel tensor where\n",
    "    1st Channel -> Whole tumor (1,2 and 4)\n",
    "    2nd Channel -> Tumor Core (1 and 4)\n",
    "    3rd Channel -> Enhancing Tumor core (4)\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, data):\n",
    "        data_dict = dict(data)\n",
    "        for key in self.keys:\n",
    "            one_hot_encode_array = [\n",
    "                torch.logical_or(\n",
    "                    torch.logical_or(data_dict[key] == 1, data_dict[key] == 2),\n",
    "                    data_dict[key] == 4,\n",
    "                ),  # Whole Tumor\n",
    "                torch.logical_or(\n",
    "                    data_dict[key] == 1, data_dict[key] == 4\n",
    "                ),  # Tumor Core\n",
    "                data_dict[key] == 4,  # Enhancing Core\n",
    "            ]\n",
    "            data_dict[key] = torch.stack(one_hot_encode_array, axis=0).astype(\n",
    "                torch.float32\n",
    "            )\n",
    "        return data_dict\n",
    "\n",
    "\n",
    "transform_no_norm = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"image\", \"seg\"]),\n",
    "        EnsureChannelFirstd(keys=[\"image\"]),\n",
    "        ConvertLabelsIntoOneHotd(keys=\"seg\"),\n",
    "        ToMetaTensord(keys=[\"image\", \"seg\"]),\n",
    "        Orientationd(keys=[\"image\", \"seg\"], axcodes=\"RAS\"),\n",
    "        #! Training Dataset Already set to Pixel Dimension of 1mm^3\n",
    "    ]\n",
    ")\n",
    "\n",
    "normalise_mri = NormalizeIntensityd(keys=\"image\", nonzero=True, channel_wise=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(input):\n",
    "        \"\"\"Do Sliding Window Inference on input tensor\n",
    "        To avoid OOM Error, Input Model done on CPU.\n",
    "        Patch taken from input and its inference done on GPU\n",
    "        to speed up inference time.\n",
    "\n",
    "        Args:\n",
    "            input: Full input to pass in the model. For the case\n",
    "            of this project size => (3,240,240,155)\n",
    "        \"\"\"\n",
    "\n",
    "        def _compute(input):\n",
    "            return sliding_window_inference(\n",
    "                inputs=input.to(\"cpu\"),\n",
    "                roi_size=(128,128,128),\n",
    "                sw_batch_size=1,\n",
    "                predictor=model,\n",
    "                overlap=0.5,\n",
    "                padding_mode=\"constant\",\n",
    "                sw_device=\"cuda:0\",\n",
    "                device=\"cpu\",\n",
    "            )\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            return _compute(input)\n",
    "\n",
    "dice_metric = DiceMetric(include_background=True, reduction=\"mean\")\n",
    "dice_metric_batch = DiceMetric(include_background=True, reduction=\"mean_batch\")\n",
    "\n",
    "hausdorff_metric = HausdorffDistanceMetric(\n",
    "    include_background=True, distance_metric='euclidean',\n",
    "    reduction=\"mean\"\n",
    ")\n",
    "\n",
    "hausdorff_metric_batch = HausdorffDistanceMetric(\n",
    "    include_background=True, distance_metric='euclidean',\n",
    "    reduction=\"mean_batch\"\n",
    ")\n",
    "post_processing_validation = Compose(\n",
    "    [Activations(sigmoid=True), AsDiscrete(threshold=0.5)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, case, TRAINING_DATASET_PATH = r\"./MICCAI_BraTS2020_TrainingData\"):\n",
    "\n",
    "    t1 = list(glob.glob(f\"{TRAINING_DATASET_PATH}/*{case:03}/*t1.nii.gz\"))\n",
    "    t1ce = list(glob.glob(f\"{TRAINING_DATASET_PATH}/*{case:03}/*t1ce.nii.gz\"))\n",
    "    t2 = list(glob.glob(f\"{TRAINING_DATASET_PATH}/*{case:03}/*t2.nii.gz\"))\n",
    "    flair = list(glob.glob(f\"{TRAINING_DATASET_PATH}/*{case:03}/*flair.nii.gz\"))\n",
    "    seg = glob.glob(f\"{TRAINING_DATASET_PATH}/*{case:03}/*seg.nii.gz\")\n",
    "\n",
    "    img = t1 + t1ce + t2 + flair\n",
    "\n",
    "    d = {\"image\": img, \"seg\": seg}\n",
    "\n",
    "    val_data_input = transform_no_norm(d)\n",
    "\n",
    "    val_data = {}\n",
    "    val_data['image'] = val_data_input['image'].detach().clone()\n",
    "    val_data['seg'] = val_data_input['seg'].detach().clone()\n",
    "    \n",
    "\n",
    "    val_data = normalise_mri(val_data)\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "\n",
    "        val_inputs, val_labels = (\n",
    "            val_data[\"image\"],\n",
    "            val_data[\"seg\"].to(device),\n",
    "        )\n",
    "\n",
    "        val_outputs = inference(torch.unsqueeze(val_inputs,0))\n",
    "        \n",
    "        val_labels = torch.unsqueeze(val_labels,0)\n",
    "\n",
    "        val_outputs_post = post_processing_validation(val_outputs.to(device))\n",
    "\n",
    "        dice_metric(y_pred=val_outputs_post, y=val_labels)\n",
    "        dice_metric_batch(y_pred=val_outputs_post, y=val_labels)\n",
    "\n",
    "        hausdorff_metric(y_pred=val_outputs_post, y=val_labels)\n",
    "        hausdorff_metric_batch(y_pred=val_outputs_post, y=val_labels)\n",
    "\n",
    "\n",
    "\n",
    "        dice_avg = dice_metric.aggregate().item()\n",
    "        dice_batch = dice_metric_batch.aggregate()\n",
    "        dice_wt = dice_batch[0].item()\n",
    "        dice_tc = dice_batch[1].item()\n",
    "        dice_et = dice_batch[2].item()\n",
    "\n",
    "        dice_metric.reset()\n",
    "\n",
    "        dice_metric_batch.reset()\n",
    "\n",
    "\n",
    "        \n",
    "        hausdorff_avg = hausdorff_metric.aggregate().item()\n",
    "        hausdorff_batch = hausdorff_metric_batch.aggregate()\n",
    "        hausdorff_wt = hausdorff_batch[0].item()\n",
    "        hausdorff_tc = hausdorff_batch[1].item()\n",
    "        hausdorff_et = hausdorff_batch[2].item()\n",
    "\n",
    "\n",
    "        hausdorff_metric.reset()\n",
    "        hausdorff_metric_batch.reset()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        return {\n",
    "            \"input\" : val_data_input,\n",
    "            \"pred\": val_outputs_post,\n",
    "            \"metrics\": {\n",
    "                \"case\" : [case],\n",
    "                \"dice_avg\": [dice_avg],\n",
    "                \"dice_wt\" : [dice_wt],\n",
    "                \"dice_tc\" :[dice_tc],\n",
    "                \"dice_et\" : [dice_et],\n",
    "                \"hdf_avg\" : [hausdorff_avg],\n",
    "                \"hdf_wt\" : [hausdorff_wt],\n",
    "                \"hdf_tc\" : [hausdorff_tc],\n",
    "                \"hdf_et\" : [hausdorff_et],\n",
    "            }\n",
    "        }\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = evaluate_model(model, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'case': [200],\n",
       " 'dice_avg': [0.89803546667099],\n",
       " 'dice_wt': [0.9219932556152344],\n",
       " 'dice_tc': [0.8866446614265442],\n",
       " 'dice_et': [0.8854685425758362],\n",
       " 'hdf_avg': [35.40738593765395],\n",
       " 'hdf_wt': [78.77182237323191],\n",
       " 'hdf_tc': [13.379088160259652],\n",
       " 'hdf_et': [14.071247279470288]}"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['metrics']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Loop automating result accumulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some Random Cases (Avoiding Duplication) -> Later expand to more based on lecturer requirements and feedback\n",
    "random_cases = np.unique(np.random.randint(0,360,30)).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ibrah\\miniconda3\\envs\\fyp\\lib\\site-packages\\monai\\metrics\\hausdorff_distance.py:168: UserWarning: the ground truth of class 2 is all 0, this may result in nan/inf distance.\n",
      "  warnings.warn(f\"the ground truth of class {c} is all 0, this may result in nan/inf distance.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing case: 289\n",
      "\n",
      "Unexpected error: <class 'IndexError'>\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[115], line 17\u001b[0m\n\u001b[0;32m     13\u001b[0m result \u001b[39m=\u001b[39m evaluate_model(model, case)\n\u001b[0;32m     15\u001b[0m metric_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat([metric_df, pd\u001b[39m.\u001b[39mDataFrame(result[\u001b[39m\"\u001b[39m\u001b[39mmetrics\u001b[39m\u001b[39m\"\u001b[39m])],ignore_index\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m---> 17\u001b[0m ground_truth \u001b[39m=\u001b[39m get_features(result[\u001b[39m\"\u001b[39;49m\u001b[39minput\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[0;32m     19\u001b[0m pred_input \u001b[39m=\u001b[39m {\n\u001b[0;32m     20\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mimage\u001b[39m\u001b[39m\"\u001b[39m: result[\u001b[39m\"\u001b[39m\u001b[39minput\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mimage\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mto(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m     21\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mseg\u001b[39m\u001b[39m\"\u001b[39m: torch\u001b[39m.\u001b[39msqueeze(result[\u001b[39m\"\u001b[39m\u001b[39mpred\u001b[39m\u001b[39m\"\u001b[39m])\u001b[39m.\u001b[39mto(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m     22\u001b[0m }\n\u001b[0;32m     24\u001b[0m predicted_features \u001b[39m=\u001b[39m get_features(pred_input)\n",
      "Cell \u001b[1;32mIn[4], line 59\u001b[0m, in \u001b[0;36mget_features\u001b[1;34m(input)\u001b[0m\n\u001b[0;32m     50\u001b[0m wt_non_txt \u001b[39m=\u001b[39m {\n\u001b[0;32m     51\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mcenter\u001b[39m\u001b[39m\"\u001b[39m: ndimage\u001b[39m.\u001b[39mcenter_of_mass(wt[blob_wt[\u001b[39m0\u001b[39m]]),  \u001b[39m# center of mass\u001b[39;00m\n\u001b[0;32m     52\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mvolume\u001b[39m\u001b[39m\"\u001b[39m: np\u001b[39m.\u001b[39mcount_nonzero(wt[blob_wt[\u001b[39m0\u001b[39m]]) \u001b[39m/\u001b[39m \u001b[39m10\u001b[39m\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m3\u001b[39m,  \u001b[39m# volume in cm^3\u001b[39;00m\n\u001b[0;32m     53\u001b[0m     }\n\u001b[0;32m     54\u001b[0m tc_non_txt \u001b[39m=\u001b[39m {\n\u001b[0;32m     55\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mcenter\u001b[39m\u001b[39m\"\u001b[39m: ndimage\u001b[39m.\u001b[39mcenter_of_mass(tc[blob_tc[\u001b[39m0\u001b[39m]]),  \u001b[39m# center of mass\u001b[39;00m\n\u001b[0;32m     56\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mvolume\u001b[39m\u001b[39m\"\u001b[39m: np\u001b[39m.\u001b[39mcount_nonzero(tc[blob_tc[\u001b[39m0\u001b[39m]]) \u001b[39m/\u001b[39m \u001b[39m10\u001b[39m\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m3\u001b[39m,  \u001b[39m# volume in cm^3\u001b[39;00m\n\u001b[0;32m     57\u001b[0m     }\n\u001b[0;32m     58\u001b[0m et_non_txt \u001b[39m=\u001b[39m {\n\u001b[1;32m---> 59\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mcenter\u001b[39m\u001b[39m\"\u001b[39m: ndimage\u001b[39m.\u001b[39mcenter_of_mass(et[blob_et[\u001b[39m0\u001b[39;49m]]),  \u001b[39m# center of mass\u001b[39;00m\n\u001b[0;32m     60\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mvolume\u001b[39m\u001b[39m\"\u001b[39m: np\u001b[39m.\u001b[39mcount_nonzero(et[blob_et[\u001b[39m0\u001b[39m]]) \u001b[39m/\u001b[39m \u001b[39m10\u001b[39m\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m3\u001b[39m,  \u001b[39m# volume in cm^3\u001b[39;00m\n\u001b[0;32m     61\u001b[0m     }\n\u001b[0;32m     63\u001b[0m feature[\u001b[39m'\u001b[39m\u001b[39mnon-textural\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m {\n\u001b[0;32m     64\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mwt\u001b[39m\u001b[39m\"\u001b[39m : wt_non_txt,\n\u001b[0;32m     65\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtc\u001b[39m\u001b[39m\"\u001b[39m : tc_non_txt,\n\u001b[0;32m     66\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39met\u001b[39m\u001b[39m\"\u001b[39m : et_non_txt\n\u001b[0;32m     67\u001b[0m }\n\u001b[0;32m     69\u001b[0m \u001b[39mreturn\u001b[39;00m feature\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Initialise DF to accumulate results\n",
    "\n",
    "metric_df = pd.DataFrame()\n",
    "features_df = pd.DataFrame()\n",
    "\n",
    "i = 0\n",
    "\n",
    "\n",
    "for case in random_cases:\n",
    "    # Do the analysis for 30\n",
    "    try:\n",
    "\n",
    "        result = evaluate_model(model, case)\n",
    "\n",
    "        metric_df = pd.concat([metric_df, pd.DataFrame(result[\"metrics\"])],ignore_index=True)\n",
    "\n",
    "        ground_truth = get_features(result[\"input\"])\n",
    "\n",
    "        pred_input = {\n",
    "            \"image\": result[\"input\"][\"image\"].to(\"cpu\"),\n",
    "            \"seg\": torch.squeeze(result[\"pred\"]).to(\"cpu\"),\n",
    "        }\n",
    "\n",
    "        predicted_features = get_features(pred_input)\n",
    "\n",
    "\n",
    "        features = pd.DataFrame(\n",
    "            {\n",
    "                \"case\": [case],\n",
    "                # Whole tumor non textural\n",
    "                \"wt_blobs_original\": [ground_truth[\"blobs\"][\"blob_wt\"]],\n",
    "                \"wt_center_original\": [ground_truth[\"non-textural\"][\"wt\"][\"center\"]],\n",
    "                \"wt_volume_original\": [ground_truth[\"non-textural\"][\"wt\"][\"volume\"]],\n",
    "                \"wt_blobs_pred\": [predicted_features[\"blobs\"][\"blob_wt\"]],\n",
    "                \"wt_center_pred\": [predicted_features[\"non-textural\"][\"wt\"][\"center\"]],\n",
    "                \"wt_volume_pred\": [predicted_features[\"non-textural\"][\"wt\"][\"volume\"]],\n",
    "                # Tumor Core non textural\n",
    "                \"tc_blobs_original\": [ground_truth[\"blobs\"][\"blob_tc\"]],\n",
    "                \"tc_center_original\": [ground_truth[\"non-textural\"][\"tc\"][\"center\"]],\n",
    "                \"tc_volume_original\": [ground_truth[\"non-textural\"][\"tc\"][\"volume\"]],\n",
    "                \"tc_blobs_pred\": [predicted_features[\"blobs\"][\"blob_tc\"]],\n",
    "                \"tc_center_pred\": [predicted_features[\"non-textural\"][\"tc\"][\"center\"]],\n",
    "                \"tc_volume_pred\": [predicted_features[\"non-textural\"][\"tc\"][\"volume\"]],\n",
    "                # Enhancing Core non textural\n",
    "                \"et_blobs_original\": [ground_truth[\"blobs\"][\"blob_et\"]],\n",
    "                \"et_center_original\": [ground_truth[\"non-textural\"][\"et\"][\"center\"]],\n",
    "                \"et_volume_original\": [ground_truth[\"non-textural\"][\"et\"][\"volume\"]],\n",
    "                \"et_blobs_pred\": [predicted_features[\"blobs\"][\"blob_et\"]],\n",
    "                \"et_center_pred\": [predicted_features[\"non-textural\"][\"et\"][\"center\"]],\n",
    "                \"et_volume_pred\": [predicted_features[\"non-textural\"][\"et\"][\"volume\"]],\n",
    "                # Textural Features:\n",
    "                ## T1\n",
    "                \"tex_energy_t1_original\": [ground_truth[\"textural\"][\"t1\"][\"energy\"]],\n",
    "                \"tex_homogeneity_t1_original\": [\n",
    "                    ground_truth[\"textural\"][\"t1\"][\"energy\"]\n",
    "                ],\n",
    "                \"tex_contrast_t1_original\": [ground_truth[\"textural\"][\"t1\"][\"energy\"]],\n",
    "                \"tex_entropy_t1_original\": [ground_truth[\"textural\"][\"t1\"][\"energy\"]],\n",
    "                \"tex_correlation_t1_original\": [\n",
    "                    ground_truth[\"textural\"][\"t1\"][\"energy\"]\n",
    "                ],\n",
    "                \"tex_energy_t1_pred\": [predicted_features[\"textural\"][\"t1\"][\"energy\"]],\n",
    "                \"tex_homogeneity_t1_pred\": [\n",
    "                    predicted_features[\"textural\"][\"t1\"][\"energy\"]\n",
    "                ],\n",
    "                \"tex_contrast_t1_pred\": [\n",
    "                    predicted_features[\"textural\"][\"t1\"][\"energy\"]\n",
    "                ],\n",
    "                \"tex_entropy_t1_pred\": [predicted_features[\"textural\"][\"t1\"][\"energy\"]],\n",
    "                \"tex_correlation_t1_pred\": [\n",
    "                    predicted_features[\"textural\"][\"t1\"][\"energy\"]\n",
    "                ],\n",
    "                ##T1CE\n",
    "                \"tex_energy_t1ce_original\": [\n",
    "                    ground_truth[\"textural\"][\"t1ce\"][\"energy\"]\n",
    "                ],\n",
    "                \"tex_homogeneity_t1ce_original\": [\n",
    "                    ground_truth[\"textural\"][\"t1ce\"][\"energy\"]\n",
    "                ],\n",
    "                \"tex_contrast_t1ce_original\": [\n",
    "                    ground_truth[\"textural\"][\"t1ce\"][\"energy\"]\n",
    "                ],\n",
    "                \"tex_entropy_t1ce_original\": [\n",
    "                    ground_truth[\"textural\"][\"t1ce\"][\"energy\"]\n",
    "                ],\n",
    "                \"tex_correlation_t1ce_original\": [\n",
    "                    ground_truth[\"textural\"][\"t1ce\"][\"energy\"]\n",
    "                ],\n",
    "                \"tex_energy_t1ce_pred\": [\n",
    "                    predicted_features[\"textural\"][\"t1ce\"][\"energy\"]\n",
    "                ],\n",
    "                \"tex_homogeneity_t1ce_pred\": [\n",
    "                    predicted_features[\"textural\"][\"t1ce\"][\"energy\"]\n",
    "                ],\n",
    "                \"tex_contrast_t1ce_pred\": [\n",
    "                    predicted_features[\"textural\"][\"t1ce\"][\"energy\"]\n",
    "                ],\n",
    "                \"tex_entropy_t1ce_pred\": [\n",
    "                    predicted_features[\"textural\"][\"t1ce\"][\"energy\"]\n",
    "                ],\n",
    "                \"tex_correlation_t1ce_pred\": [\n",
    "                    predicted_features[\"textural\"][\"t1ce\"][\"energy\"]\n",
    "                ],\n",
    "                ##T2\n",
    "                \"tex_energy_t2_original\": [ground_truth[\"textural\"][\"t2\"][\"energy\"]],\n",
    "                \"tex_homogeneity_t2_original\": [\n",
    "                    ground_truth[\"textural\"][\"t2\"][\"energy\"]\n",
    "                ],\n",
    "                \"tex_contrast_t2_original\": [ground_truth[\"textural\"][\"t2\"][\"energy\"]],\n",
    "                \"tex_entropy_t2_original\": [ground_truth[\"textural\"][\"t2\"][\"energy\"]],\n",
    "                \"tex_correlation_t2_original\": [\n",
    "                    ground_truth[\"textural\"][\"t2\"][\"energy\"]\n",
    "                ],\n",
    "                \"tex_energy_t2_pred\": [predicted_features[\"textural\"][\"t2\"][\"energy\"]],\n",
    "                \"tex_homogeneity_t2_pred\": [\n",
    "                    predicted_features[\"textural\"][\"t2\"][\"energy\"]\n",
    "                ],\n",
    "                \"tex_contrast_t2_pred\": [\n",
    "                    predicted_features[\"textural\"][\"t2\"][\"energy\"]\n",
    "                ],\n",
    "                \"tex_entropy_t2_pred\": [predicted_features[\"textural\"][\"t2\"][\"energy\"]],\n",
    "                \"tex_correlation_t2_pred\": [\n",
    "                    predicted_features[\"textural\"][\"t2\"][\"energy\"]\n",
    "                ],\n",
    "                ##FLAIR\n",
    "                \"tex_energy_flair_original\": [\n",
    "                    ground_truth[\"textural\"][\"flair\"][\"energy\"]\n",
    "                ],\n",
    "                \"tex_homogeneity_flair_original\": [\n",
    "                    ground_truth[\"textural\"][\"flair\"][\"energy\"]\n",
    "                ],\n",
    "                \"tex_contrast_flair_original\": [\n",
    "                    ground_truth[\"textural\"][\"flair\"][\"energy\"]\n",
    "                ],\n",
    "                \"tex_entropy_flair_original\": [\n",
    "                    ground_truth[\"textural\"][\"flair\"][\"energy\"]\n",
    "                ],\n",
    "                \"tex_correlation_flair_original\": [\n",
    "                    ground_truth[\"textural\"][\"flair\"][\"energy\"]\n",
    "                ],\n",
    "                \"tex_energy_flair_pred\": [\n",
    "                    predicted_features[\"textural\"][\"flair\"][\"energy\"]\n",
    "                ],\n",
    "                \"tex_homogeneity_flair_pred\": [\n",
    "                    predicted_features[\"textural\"][\"flair\"][\"energy\"]\n",
    "                ],\n",
    "                \"tex_contrast_flair_pred\": [\n",
    "                    predicted_features[\"textural\"][\"flair\"][\"energy\"]\n",
    "                ],\n",
    "                \"tex_entropy_flair_pred\": [\n",
    "                    predicted_features[\"textural\"][\"flair\"][\"energy\"]\n",
    "                ],\n",
    "                \"tex_correlation_flair_pred\": [\n",
    "                    predicted_features[\"textural\"][\"flair\"][\"energy\"]\n",
    "                ],\n",
    "            }\n",
    "        )\n",
    "\n",
    "\n",
    "        features_df = pd.concat([features_df, features],ignore_index=True)\n",
    "\n",
    "    except:\n",
    "        print(f\"Error processing case: {case}\\n\")\n",
    "        print(\"Unexpected error:\", sys.exc_info()[0])\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_df.sort_values(by='case',inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DICE Scores and Hausdaurff Distance for Each Evaluated Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case</th>\n",
       "      <th>dice_avg</th>\n",
       "      <th>dice_wt</th>\n",
       "      <th>dice_tc</th>\n",
       "      <th>dice_et</th>\n",
       "      <th>hdf_avg</th>\n",
       "      <th>hdf_wt</th>\n",
       "      <th>hdf_tc</th>\n",
       "      <th>hdf_et</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0.833346</td>\n",
       "      <td>0.732529</td>\n",
       "      <td>0.916864</td>\n",
       "      <td>0.850645</td>\n",
       "      <td>29.954033</td>\n",
       "      <td>76.032888</td>\n",
       "      <td>6.480741</td>\n",
       "      <td>7.348469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30</td>\n",
       "      <td>0.806884</td>\n",
       "      <td>0.826244</td>\n",
       "      <td>0.812753</td>\n",
       "      <td>0.781653</td>\n",
       "      <td>40.005474</td>\n",
       "      <td>69.028979</td>\n",
       "      <td>24.738634</td>\n",
       "      <td>26.248809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36</td>\n",
       "      <td>0.845233</td>\n",
       "      <td>0.827933</td>\n",
       "      <td>0.885761</td>\n",
       "      <td>0.822005</td>\n",
       "      <td>32.276219</td>\n",
       "      <td>86.272823</td>\n",
       "      <td>5.656854</td>\n",
       "      <td>4.898979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>42</td>\n",
       "      <td>0.902013</td>\n",
       "      <td>0.939058</td>\n",
       "      <td>0.930642</td>\n",
       "      <td>0.836338</td>\n",
       "      <td>22.597714</td>\n",
       "      <td>51.361464</td>\n",
       "      <td>10.954451</td>\n",
       "      <td>5.477226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>47</td>\n",
       "      <td>0.879518</td>\n",
       "      <td>0.878071</td>\n",
       "      <td>0.925334</td>\n",
       "      <td>0.835148</td>\n",
       "      <td>51.255711</td>\n",
       "      <td>92.010869</td>\n",
       "      <td>31.080541</td>\n",
       "      <td>30.675723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>61</td>\n",
       "      <td>0.820444</td>\n",
       "      <td>0.761602</td>\n",
       "      <td>0.908783</td>\n",
       "      <td>0.790947</td>\n",
       "      <td>18.081069</td>\n",
       "      <td>45.188494</td>\n",
       "      <td>4.582576</td>\n",
       "      <td>4.472136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>62</td>\n",
       "      <td>0.925625</td>\n",
       "      <td>0.933668</td>\n",
       "      <td>0.951146</td>\n",
       "      <td>0.892061</td>\n",
       "      <td>8.544027</td>\n",
       "      <td>11.045361</td>\n",
       "      <td>10.344080</td>\n",
       "      <td>4.242641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>67</td>\n",
       "      <td>0.926374</td>\n",
       "      <td>0.941353</td>\n",
       "      <td>0.952307</td>\n",
       "      <td>0.885461</td>\n",
       "      <td>24.331659</td>\n",
       "      <td>56.480085</td>\n",
       "      <td>10.770330</td>\n",
       "      <td>5.744563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>86</td>\n",
       "      <td>0.672317</td>\n",
       "      <td>0.720867</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.653226</td>\n",
       "      <td>38.903515</td>\n",
       "      <td>105.280578</td>\n",
       "      <td>4.358899</td>\n",
       "      <td>7.071068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>90</td>\n",
       "      <td>0.916448</td>\n",
       "      <td>0.894363</td>\n",
       "      <td>0.951745</td>\n",
       "      <td>0.903237</td>\n",
       "      <td>12.688307</td>\n",
       "      <td>27.676705</td>\n",
       "      <td>5.916080</td>\n",
       "      <td>4.472136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>92</td>\n",
       "      <td>0.922109</td>\n",
       "      <td>0.921458</td>\n",
       "      <td>0.934361</td>\n",
       "      <td>0.910509</td>\n",
       "      <td>10.495276</td>\n",
       "      <td>13.928388</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>6.557439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>94</td>\n",
       "      <td>0.903087</td>\n",
       "      <td>0.911046</td>\n",
       "      <td>0.900314</td>\n",
       "      <td>0.897902</td>\n",
       "      <td>19.676691</td>\n",
       "      <td>55.883808</td>\n",
       "      <td>1.414214</td>\n",
       "      <td>1.732051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>95</td>\n",
       "      <td>0.898138</td>\n",
       "      <td>0.961920</td>\n",
       "      <td>0.866315</td>\n",
       "      <td>0.866179</td>\n",
       "      <td>10.676422</td>\n",
       "      <td>16.031220</td>\n",
       "      <td>7.874008</td>\n",
       "      <td>8.124038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>100</td>\n",
       "      <td>0.887189</td>\n",
       "      <td>0.951635</td>\n",
       "      <td>0.853994</td>\n",
       "      <td>0.855937</td>\n",
       "      <td>20.714825</td>\n",
       "      <td>33.271610</td>\n",
       "      <td>18.384776</td>\n",
       "      <td>10.488088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>130</td>\n",
       "      <td>0.901197</td>\n",
       "      <td>0.883162</td>\n",
       "      <td>0.947929</td>\n",
       "      <td>0.872501</td>\n",
       "      <td>7.344212</td>\n",
       "      <td>9.949874</td>\n",
       "      <td>6.082763</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>157</td>\n",
       "      <td>0.882621</td>\n",
       "      <td>0.886995</td>\n",
       "      <td>0.884838</td>\n",
       "      <td>0.876030</td>\n",
       "      <td>26.255234</td>\n",
       "      <td>57.428216</td>\n",
       "      <td>11.090537</td>\n",
       "      <td>10.246951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>175</td>\n",
       "      <td>0.912146</td>\n",
       "      <td>0.940106</td>\n",
       "      <td>0.909942</td>\n",
       "      <td>0.886389</td>\n",
       "      <td>21.878383</td>\n",
       "      <td>39.673669</td>\n",
       "      <td>12.961481</td>\n",
       "      <td>13.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>180</td>\n",
       "      <td>0.891609</td>\n",
       "      <td>0.793631</td>\n",
       "      <td>0.952157</td>\n",
       "      <td>0.929037</td>\n",
       "      <td>43.893018</td>\n",
       "      <td>63.158531</td>\n",
       "      <td>60.646517</td>\n",
       "      <td>7.874008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>201</td>\n",
       "      <td>0.925714</td>\n",
       "      <td>0.949620</td>\n",
       "      <td>0.950942</td>\n",
       "      <td>0.876579</td>\n",
       "      <td>33.736826</td>\n",
       "      <td>37.255872</td>\n",
       "      <td>31.304952</td>\n",
       "      <td>32.649655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>223</td>\n",
       "      <td>0.958388</td>\n",
       "      <td>0.969876</td>\n",
       "      <td>0.961973</td>\n",
       "      <td>0.943314</td>\n",
       "      <td>31.041747</td>\n",
       "      <td>77.807455</td>\n",
       "      <td>9.486833</td>\n",
       "      <td>5.830952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>247</td>\n",
       "      <td>0.910069</td>\n",
       "      <td>0.893013</td>\n",
       "      <td>0.946354</td>\n",
       "      <td>0.890839</td>\n",
       "      <td>32.118189</td>\n",
       "      <td>82.006097</td>\n",
       "      <td>7.348469</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>252</td>\n",
       "      <td>0.917948</td>\n",
       "      <td>0.959107</td>\n",
       "      <td>0.917212</td>\n",
       "      <td>0.877526</td>\n",
       "      <td>13.647758</td>\n",
       "      <td>11.704700</td>\n",
       "      <td>19.339080</td>\n",
       "      <td>9.899495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>253</td>\n",
       "      <td>0.881074</td>\n",
       "      <td>0.953234</td>\n",
       "      <td>0.944497</td>\n",
       "      <td>0.745491</td>\n",
       "      <td>8.408265</td>\n",
       "      <td>11.445523</td>\n",
       "      <td>6.708204</td>\n",
       "      <td>7.071068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>289</td>\n",
       "      <td>0.782577</td>\n",
       "      <td>0.919151</td>\n",
       "      <td>0.646003</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>inf</td>\n",
       "      <td>44.011362</td>\n",
       "      <td>16.583124</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>294</td>\n",
       "      <td>0.808753</td>\n",
       "      <td>0.918395</td>\n",
       "      <td>0.699110</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>inf</td>\n",
       "      <td>10.630146</td>\n",
       "      <td>12.083046</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>301</td>\n",
       "      <td>0.720265</td>\n",
       "      <td>0.856218</td>\n",
       "      <td>0.684919</td>\n",
       "      <td>0.619658</td>\n",
       "      <td>31.654974</td>\n",
       "      <td>53.422842</td>\n",
       "      <td>21.023796</td>\n",
       "      <td>20.518285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>320</td>\n",
       "      <td>0.796104</td>\n",
       "      <td>0.761721</td>\n",
       "      <td>0.807997</td>\n",
       "      <td>0.818592</td>\n",
       "      <td>101.010369</td>\n",
       "      <td>94.392796</td>\n",
       "      <td>104.201727</td>\n",
       "      <td>104.436584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>321</td>\n",
       "      <td>0.820150</td>\n",
       "      <td>0.930249</td>\n",
       "      <td>0.710050</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>51.866340</td>\n",
       "      <td>92.330927</td>\n",
       "      <td>11.401754</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>352</td>\n",
       "      <td>0.919143</td>\n",
       "      <td>0.931877</td>\n",
       "      <td>0.929537</td>\n",
       "      <td>0.896015</td>\n",
       "      <td>28.989078</td>\n",
       "      <td>80.361682</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.605551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>353</td>\n",
       "      <td>0.914912</td>\n",
       "      <td>0.947695</td>\n",
       "      <td>0.912024</td>\n",
       "      <td>0.885016</td>\n",
       "      <td>38.535901</td>\n",
       "      <td>101.616928</td>\n",
       "      <td>8.246211</td>\n",
       "      <td>5.744563</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    case  dice_avg   dice_wt   dice_tc   dice_et     hdf_avg      hdf_wt  \\\n",
       "0      3  0.833346  0.732529  0.916864  0.850645   29.954033   76.032888   \n",
       "1     30  0.806884  0.826244  0.812753  0.781653   40.005474   69.028979   \n",
       "2     36  0.845233  0.827933  0.885761  0.822005   32.276219   86.272823   \n",
       "3     42  0.902013  0.939058  0.930642  0.836338   22.597714   51.361464   \n",
       "4     47  0.879518  0.878071  0.925334  0.835148   51.255711   92.010869   \n",
       "5     61  0.820444  0.761602  0.908783  0.790947   18.081069   45.188494   \n",
       "6     62  0.925625  0.933668  0.951146  0.892061    8.544027   11.045361   \n",
       "7     67  0.926374  0.941353  0.952307  0.885461   24.331659   56.480085   \n",
       "8     86  0.672317  0.720867  0.642857  0.653226   38.903515  105.280578   \n",
       "9     90  0.916448  0.894363  0.951745  0.903237   12.688307   27.676705   \n",
       "10    92  0.922109  0.921458  0.934361  0.910509   10.495276   13.928388   \n",
       "11    94  0.903087  0.911046  0.900314  0.897902   19.676691   55.883808   \n",
       "12    95  0.898138  0.961920  0.866315  0.866179   10.676422   16.031220   \n",
       "13   100  0.887189  0.951635  0.853994  0.855937   20.714825   33.271610   \n",
       "14   130  0.901197  0.883162  0.947929  0.872501    7.344212    9.949874   \n",
       "15   157  0.882621  0.886995  0.884838  0.876030   26.255234   57.428216   \n",
       "16   175  0.912146  0.940106  0.909942  0.886389   21.878383   39.673669   \n",
       "17   180  0.891609  0.793631  0.952157  0.929037   43.893018   63.158531   \n",
       "18   201  0.925714  0.949620  0.950942  0.876579   33.736826   37.255872   \n",
       "19   223  0.958388  0.969876  0.961973  0.943314   31.041747   77.807455   \n",
       "20   247  0.910069  0.893013  0.946354  0.890839   32.118189   82.006097   \n",
       "21   252  0.917948  0.959107  0.917212  0.877526   13.647758   11.704700   \n",
       "22   253  0.881074  0.953234  0.944497  0.745491    8.408265   11.445523   \n",
       "23   289  0.782577  0.919151  0.646003  0.000000         inf   44.011362   \n",
       "24   294  0.808753  0.918395  0.699110  0.000000         inf   10.630146   \n",
       "25   301  0.720265  0.856218  0.684919  0.619658   31.654974   53.422842   \n",
       "26   320  0.796104  0.761721  0.807997  0.818592  101.010369   94.392796   \n",
       "27   321  0.820150  0.930249  0.710050  0.000000   51.866340   92.330927   \n",
       "28   352  0.919143  0.931877  0.929537  0.896015   28.989078   80.361682   \n",
       "29   353  0.914912  0.947695  0.912024  0.885016   38.535901  101.616928   \n",
       "\n",
       "        hdf_tc      hdf_et  \n",
       "0     6.480741    7.348469  \n",
       "1    24.738634   26.248809  \n",
       "2     5.656854    4.898979  \n",
       "3    10.954451    5.477226  \n",
       "4    31.080541   30.675723  \n",
       "5     4.582576    4.472136  \n",
       "6    10.344080    4.242641  \n",
       "7    10.770330    5.744563  \n",
       "8     4.358899    7.071068  \n",
       "9     5.916080    4.472136  \n",
       "10   11.000000    6.557439  \n",
       "11    1.414214    1.732051  \n",
       "12    7.874008    8.124038  \n",
       "13   18.384776   10.488088  \n",
       "14    6.082763    6.000000  \n",
       "15   11.090537   10.246951  \n",
       "16   12.961481   13.000000  \n",
       "17   60.646517    7.874008  \n",
       "18   31.304952   32.649655  \n",
       "19    9.486833    5.830952  \n",
       "20    7.348469    7.000000  \n",
       "21   19.339080    9.899495  \n",
       "22    6.708204    7.071068  \n",
       "23   16.583124         inf  \n",
       "24   12.083046         inf  \n",
       "25   21.023796   20.518285  \n",
       "26  104.201727  104.436584  \n",
       "27   11.401754    0.000000  \n",
       "28    3.000000    3.605551  \n",
       "29    8.246211    5.744563  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dice_avg</th>\n",
       "      <th>dice_wt</th>\n",
       "      <th>dice_tc</th>\n",
       "      <th>dice_et</th>\n",
       "      <th>hdf_avg</th>\n",
       "      <th>hdf_wt</th>\n",
       "      <th>hdf_tc</th>\n",
       "      <th>hdf_et</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>23.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>23.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.883456</td>\n",
       "      <td>0.888282</td>\n",
       "      <td>0.906479</td>\n",
       "      <td>0.855607</td>\n",
       "      <td>24.283677</td>\n",
       "      <td>49.127096</td>\n",
       "      <td>13.848914</td>\n",
       "      <td>9.875022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.058803</td>\n",
       "      <td>0.075626</td>\n",
       "      <td>0.068617</td>\n",
       "      <td>0.064306</td>\n",
       "      <td>12.358090</td>\n",
       "      <td>29.281668</td>\n",
       "      <td>13.022314</td>\n",
       "      <td>8.331569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.672317</td>\n",
       "      <td>0.720867</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.653226</td>\n",
       "      <td>7.344212</td>\n",
       "      <td>9.949874</td>\n",
       "      <td>1.414214</td>\n",
       "      <td>1.732051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.880296</td>\n",
       "      <td>0.853002</td>\n",
       "      <td>0.893038</td>\n",
       "      <td>0.835743</td>\n",
       "      <td>13.168033</td>\n",
       "      <td>21.853962</td>\n",
       "      <td>6.281752</td>\n",
       "      <td>5.610894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.901197</td>\n",
       "      <td>0.911046</td>\n",
       "      <td>0.925334</td>\n",
       "      <td>0.876030</td>\n",
       "      <td>22.597714</td>\n",
       "      <td>51.361464</td>\n",
       "      <td>10.344080</td>\n",
       "      <td>7.071068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.917198</td>\n",
       "      <td>0.945486</td>\n",
       "      <td>0.949435</td>\n",
       "      <td>0.891450</td>\n",
       "      <td>32.197204</td>\n",
       "      <td>72.530934</td>\n",
       "      <td>15.673129</td>\n",
       "      <td>10.073223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.958388</td>\n",
       "      <td>0.969876</td>\n",
       "      <td>0.961973</td>\n",
       "      <td>0.943314</td>\n",
       "      <td>51.255711</td>\n",
       "      <td>105.280578</td>\n",
       "      <td>60.646517</td>\n",
       "      <td>32.649655</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        dice_avg    dice_wt    dice_tc    dice_et    hdf_avg      hdf_wt  \\\n",
       "count  23.000000  23.000000  23.000000  23.000000  23.000000   23.000000   \n",
       "mean    0.883456   0.888282   0.906479   0.855607  24.283677   49.127096   \n",
       "std     0.058803   0.075626   0.068617   0.064306  12.358090   29.281668   \n",
       "min     0.672317   0.720867   0.642857   0.653226   7.344212    9.949874   \n",
       "25%     0.880296   0.853002   0.893038   0.835743  13.168033   21.853962   \n",
       "50%     0.901197   0.911046   0.925334   0.876030  22.597714   51.361464   \n",
       "75%     0.917198   0.945486   0.949435   0.891450  32.197204   72.530934   \n",
       "max     0.958388   0.969876   0.961973   0.943314  51.255711  105.280578   \n",
       "\n",
       "          hdf_tc     hdf_et  \n",
       "count  23.000000  23.000000  \n",
       "mean   13.848914   9.875022  \n",
       "std    13.022314   8.331569  \n",
       "min     1.414214   1.732051  \n",
       "25%     6.281752   5.610894  \n",
       "50%    10.344080   7.071068  \n",
       "75%    15.673129  10.073223  \n",
       "max    60.646517  32.649655  "
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_df.iloc[:,1:][metric_df.dice_et > 0 ].describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Textural and Non Textural Comparison between ground truth and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4 entries, 0 to 3\n",
      "Data columns (total 59 columns):\n",
      " #   Column                          Non-Null Count  Dtype  \n",
      "---  ------                          --------------  -----  \n",
      " 0   case                            4 non-null      int64  \n",
      " 1   wt_blobs_original               4 non-null      int64  \n",
      " 2   wt_center_original              4 non-null      object \n",
      " 3   wt_volume_original              4 non-null      float64\n",
      " 4   wt_blobs_pred                   4 non-null      int64  \n",
      " 5   wt_center_pred                  4 non-null      object \n",
      " 6   wt_volume_pred                  4 non-null      float64\n",
      " 7   tc_blobs_original               4 non-null      int64  \n",
      " 8   tc_center_original              4 non-null      object \n",
      " 9   tc_volume_original              4 non-null      float64\n",
      " 10  tc_blobs_pred                   4 non-null      int64  \n",
      " 11  tc_center_pred                  4 non-null      object \n",
      " 12  tc_volume_pred                  4 non-null      float64\n",
      " 13  et_blobs_original               4 non-null      int64  \n",
      " 14  et_center_original              4 non-null      object \n",
      " 15  et_volume_original              4 non-null      float64\n",
      " 16  et_blobs_pred                   4 non-null      int64  \n",
      " 17  et_center_pred                  4 non-null      object \n",
      " 18  et_volume_pred                  4 non-null      float64\n",
      " 19  tex_energy_t1_original          4 non-null      float64\n",
      " 20  tex_homogeneity_t1_original     4 non-null      float64\n",
      " 21  tex_contrast_t1_original        4 non-null      float64\n",
      " 22  tex_entropy_t1_original         4 non-null      float64\n",
      " 23  tex_correlation_t1_original     4 non-null      float64\n",
      " 24  tex_energy_t1_pred              4 non-null      float64\n",
      " 25  tex_homogeneity_t1_pred         4 non-null      float64\n",
      " 26  tex_contrast_t1_pred            4 non-null      float64\n",
      " 27  tex_entropy_t1_pred             4 non-null      float64\n",
      " 28  tex_correlation_t1_pred         4 non-null      float64\n",
      " 29  tex_energy_t1ce_original        4 non-null      float64\n",
      " 30  tex_homogeneity_t1ce_original   4 non-null      float64\n",
      " 31  tex_contrast_t1ce_original      4 non-null      float64\n",
      " 32  tex_entropy_t1ce_original       4 non-null      float64\n",
      " 33  tex_correlation_t1ce_original   4 non-null      float64\n",
      " 34  tex_energy_t1ce_pred            4 non-null      float64\n",
      " 35  tex_homogeneity_t1ce_pred       4 non-null      float64\n",
      " 36  tex_contrast_t1ce_pred          4 non-null      float64\n",
      " 37  tex_entropy_t1ce_pred           4 non-null      float64\n",
      " 38  tex_correlation_t1ce_pred       4 non-null      float64\n",
      " 39  tex_energy_t2_original          4 non-null      float64\n",
      " 40  tex_homogeneity_t2_original     4 non-null      float64\n",
      " 41  tex_contrast_t2_original        4 non-null      float64\n",
      " 42  tex_entropy_t2_original         4 non-null      float64\n",
      " 43  tex_correlation_t2_original     4 non-null      float64\n",
      " 44  tex_energy_t2_pred              4 non-null      float64\n",
      " 45  tex_homogeneity_t2_pred         4 non-null      float64\n",
      " 46  tex_contrast_t2_pred            4 non-null      float64\n",
      " 47  tex_entropy_t2_pred             4 non-null      float64\n",
      " 48  tex_correlation_t2_pred         4 non-null      float64\n",
      " 49  tex_energy_flair_original       4 non-null      float64\n",
      " 50  tex_homogeneity_flair_original  4 non-null      float64\n",
      " 51  tex_contrast_flair_original     4 non-null      float64\n",
      " 52  tex_entropy_flair_original      4 non-null      float64\n",
      " 53  tex_correlation_flair_original  4 non-null      float64\n",
      " 54  tex_energy_flair_pred           4 non-null      float64\n",
      " 55  tex_homogeneity_flair_pred      4 non-null      float64\n",
      " 56  tex_contrast_flair_pred         4 non-null      float64\n",
      " 57  tex_entropy_flair_pred          4 non-null      float64\n",
      " 58  tex_correlation_flair_pred      4 non-null      float64\n",
      "dtypes: float64(46), int64(7), object(6)\n",
      "memory usage: 2.0+ KB\n"
     ]
    }
   ],
   "source": [
    "features_df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Difference in Number of Blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    27.0\n",
       "mean      0.0\n",
       "std       0.0\n",
       "min       0.0\n",
       "25%       0.0\n",
       "50%       0.0\n",
       "75%       0.0\n",
       "max       0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_blobs_wt = features_df.wt_blobs_original - features_df.wt_blobs_pred\n",
    "diff_blobs_wt.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    27.0\n",
       "mean      0.0\n",
       "std       0.0\n",
       "min       0.0\n",
       "25%       0.0\n",
       "50%       0.0\n",
       "75%       0.0\n",
       "max       0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_blobs_tc = features_df.tc_blobs_original - features_df.tc_blobs_pred\n",
    "diff_blobs_tc.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    27.0\n",
       "mean      0.0\n",
       "std       0.0\n",
       "min       0.0\n",
       "25%       0.0\n",
       "50%       0.0\n",
       "75%       0.0\n",
       "max       0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_blobs_et = features_df.et_blobs_original - features_df.et_blobs_pred\n",
    "diff_blobs_et.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of Center point\n",
    "\n",
    "Only Largest Blob taken as in the off chance that there are 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_between_midpoints(p1, p2):\n",
    "            \"\"\"Given 2 3D points calculate euclidean distance between them.\n",
    "            Coordinates in (x,y,z)\n",
    "\n",
    "            Args:\n",
    "                p1 (tuple): point 1\n",
    "                p2 (tuple): point 2\n",
    "\n",
    "            Returns:\n",
    "                _type_: distance\n",
    "            \"\"\"\n",
    "            return  np.sqrt(p2[0] - p1[0] ** 2 + (p2[1] - p1[1]) ** 2 + (p2[2] - p1[2]) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    27.000000\n",
       "mean     32.962659\n",
       "std      30.723328\n",
       "min       0.888649\n",
       "25%       3.900877\n",
       "50%      30.689826\n",
       "75%      47.801052\n",
       "max      98.624723\n",
       "dtype: float64"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wt_center_offset = features_df.apply(\n",
    "    lambda row: distance_between_midpoints(row['wt_center_original'], row['wt_center_pred']), axis=1\n",
    ")\n",
    "wt_center_offset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     27.000000\n",
       "mean       9.011370\n",
       "std       22.928087\n",
       "min        0.345896\n",
       "25%        1.332717\n",
       "50%        2.078482\n",
       "75%        2.877048\n",
       "max      111.427613\n",
       "dtype: float64"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tc_center_offset = features_df.apply(\n",
    "    lambda row: distance_between_midpoints(row['tc_center_original'], row['tc_center_pred']), axis=1\n",
    ")\n",
    "tc_center_offset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     27.000000\n",
       "mean       7.315402\n",
       "std       21.358199\n",
       "min        0.398660\n",
       "25%        1.440253\n",
       "50%        2.005945\n",
       "75%        3.157804\n",
       "max      112.412641\n",
       "dtype: float64"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "et_center_offset = features_df.apply(\n",
    "    lambda row: distance_between_midpoints(row['et_center_original'], row['et_center_pred']), axis=1\n",
    ")\n",
    "et_center_offset.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of Difference in Tumor Volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    27.000000\n",
       "mean      8.699222\n",
       "std      11.799127\n",
       "min       0.193000\n",
       "25%       1.548500\n",
       "50%       3.886000\n",
       "75%      11.686500\n",
       "max      53.542000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_volume_wt = np.abs(features_df.wt_volume_original - features_df.wt_volume_pred)\n",
    "diff_volume_wt.describe()       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     27.000000\n",
       "mean      57.325815\n",
       "std       39.362460\n",
       "min       11.352000\n",
       "25%       26.218000\n",
       "50%       42.122000\n",
       "75%       82.659000\n",
       "max      136.630000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_volume_tc = np.abs(features_df.tc_volume_original - features_df.wt_volume_pred)\n",
    "diff_volume_tc.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     27.000000\n",
       "mean      71.631074\n",
       "std       45.735554\n",
       "min       13.799000\n",
       "25%       30.142500\n",
       "50%       63.535000\n",
       "75%      104.856500\n",
       "max      153.745000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_volume_et = np.abs(features_df.et_volume_original - features_df.wt_volume_pred)\n",
    "diff_volume_et.describe()   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of % difference of textural features.\n",
    "\n",
    "$$\\frac{original-pred}{original}\\cdot100$$\n",
    "\n",
    "Direction Vector Used was $(1,1,1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_tex_t1 = pd.DataFrame()\n",
    "diff_tex_t1.loc[:,\"Energy\"] = np.abs(features_df.tex_energy_t1_original - features_df.tex_energy_t1_pred) / features_df.tex_energy_t1_original * 100\n",
    "\n",
    "\n",
    "diff_tex_t1.loc[:,\"Homogeneity\"] = np.abs(features_df.tex_homogeneity_t1_original - features_df.tex_homogeneity_t1_pred) / features_df.tex_homogeneity_t1_original * 100\n",
    "\n",
    "diff_tex_t1.loc[:,\"Contrast\"] = np.abs(features_df.tex_contrast_t1_original - features_df.tex_contrast_t1_pred) / features_df.tex_contrast_t1_original * 100\n",
    "\n",
    "diff_tex_t1.loc[:,\"Entropy\"] = np.abs(features_df.tex_entropy_t1_original - features_df.tex_entropy_t1_pred) / features_df.tex_entropy_t1_original * 100\n",
    "\n",
    "diff_tex_t1.loc[:,\"Correlation\"] = np.abs(features_df.tex_correlation_t1_original - features_df.tex_correlation_t1_pred) / features_df.tex_correlation_t1_original * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Energy</th>\n",
       "      <th>Homogeneity</th>\n",
       "      <th>Contrast</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Correlation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>287.396657</td>\n",
       "      <td>287.396657</td>\n",
       "      <td>287.396657</td>\n",
       "      <td>287.396657</td>\n",
       "      <td>287.396657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>726.020139</td>\n",
       "      <td>726.020139</td>\n",
       "      <td>726.020139</td>\n",
       "      <td>726.020139</td>\n",
       "      <td>726.020139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.935273</td>\n",
       "      <td>1.935273</td>\n",
       "      <td>1.935273</td>\n",
       "      <td>1.935273</td>\n",
       "      <td>1.935273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>13.390527</td>\n",
       "      <td>13.390527</td>\n",
       "      <td>13.390527</td>\n",
       "      <td>13.390527</td>\n",
       "      <td>13.390527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>25.406454</td>\n",
       "      <td>25.406454</td>\n",
       "      <td>25.406454</td>\n",
       "      <td>25.406454</td>\n",
       "      <td>25.406454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>103.683027</td>\n",
       "      <td>103.683027</td>\n",
       "      <td>103.683027</td>\n",
       "      <td>103.683027</td>\n",
       "      <td>103.683027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2785.197446</td>\n",
       "      <td>2785.197446</td>\n",
       "      <td>2785.197446</td>\n",
       "      <td>2785.197446</td>\n",
       "      <td>2785.197446</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Energy  Homogeneity     Contrast      Entropy  Correlation\n",
       "count    27.000000    27.000000    27.000000    27.000000    27.000000\n",
       "mean    287.396657   287.396657   287.396657   287.396657   287.396657\n",
       "std     726.020139   726.020139   726.020139   726.020139   726.020139\n",
       "min       1.935273     1.935273     1.935273     1.935273     1.935273\n",
       "25%      13.390527    13.390527    13.390527    13.390527    13.390527\n",
       "50%      25.406454    25.406454    25.406454    25.406454    25.406454\n",
       "75%     103.683027   103.683027   103.683027   103.683027   103.683027\n",
       "max    2785.197446  2785.197446  2785.197446  2785.197446  2785.197446"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_tex_t1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_tex_t1ce = pd.DataFrame()\n",
    "diff_tex_t1ce.loc[:,\"Energy\"] = np.abs(features_df.tex_energy_t1ce_original - features_df.tex_energy_t1ce_pred) / features_df.tex_energy_t1ce_original * 100\n",
    "\n",
    "\n",
    "diff_tex_t1ce.loc[:,\"Homogeneity\"] = np.abs(features_df.tex_homogeneity_t1ce_original - features_df.tex_homogeneity_t1ce_pred) / features_df.tex_homogeneity_t1ce_original * 100\n",
    "\n",
    "diff_tex_t1ce.loc[:,\"Contrast\"] = np.abs(features_df.tex_contrast_t1ce_original - features_df.tex_contrast_t1ce_pred) / features_df.tex_contrast_t1ce_original * 100\n",
    "\n",
    "diff_tex_t1ce.loc[:,\"Entropy\"] = np.abs(features_df.tex_entropy_t1ce_original - features_df.tex_entropy_t1ce_pred) / features_df.tex_entropy_t1ce_original * 100\n",
    "\n",
    "diff_tex_t1ce.loc[:,\"Correlation\"] = np.abs(features_df.tex_correlation_t1ce_original - features_df.tex_correlation_t1ce_pred) / features_df.tex_correlation_t1ce_original * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Energy</th>\n",
       "      <th>Homogeneity</th>\n",
       "      <th>Contrast</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Correlation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>289.128447</td>\n",
       "      <td>289.128447</td>\n",
       "      <td>289.128447</td>\n",
       "      <td>289.128447</td>\n",
       "      <td>289.128447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>731.341176</td>\n",
       "      <td>731.341176</td>\n",
       "      <td>731.341176</td>\n",
       "      <td>731.341176</td>\n",
       "      <td>731.341176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.936703</td>\n",
       "      <td>1.936703</td>\n",
       "      <td>1.936703</td>\n",
       "      <td>1.936703</td>\n",
       "      <td>1.936703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>14.010748</td>\n",
       "      <td>14.010748</td>\n",
       "      <td>14.010748</td>\n",
       "      <td>14.010748</td>\n",
       "      <td>14.010748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>24.922507</td>\n",
       "      <td>24.922507</td>\n",
       "      <td>24.922507</td>\n",
       "      <td>24.922507</td>\n",
       "      <td>24.922507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>104.049017</td>\n",
       "      <td>104.049017</td>\n",
       "      <td>104.049017</td>\n",
       "      <td>104.049017</td>\n",
       "      <td>104.049017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2818.279044</td>\n",
       "      <td>2818.279044</td>\n",
       "      <td>2818.279044</td>\n",
       "      <td>2818.279044</td>\n",
       "      <td>2818.279044</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Energy  Homogeneity     Contrast      Entropy  Correlation\n",
       "count    27.000000    27.000000    27.000000    27.000000    27.000000\n",
       "mean    289.128447   289.128447   289.128447   289.128447   289.128447\n",
       "std     731.341176   731.341176   731.341176   731.341176   731.341176\n",
       "min       1.936703     1.936703     1.936703     1.936703     1.936703\n",
       "25%      14.010748    14.010748    14.010748    14.010748    14.010748\n",
       "50%      24.922507    24.922507    24.922507    24.922507    24.922507\n",
       "75%     104.049017   104.049017   104.049017   104.049017   104.049017\n",
       "max    2818.279044  2818.279044  2818.279044  2818.279044  2818.279044"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_tex_t1ce.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_tex_t2 = pd.DataFrame()\n",
    "diff_tex_t2.loc[:,\"Energy\"] = np.abs(features_df.tex_energy_t2_original - features_df.tex_energy_t2_pred) / features_df.tex_energy_t2_original * 100\n",
    "\n",
    "\n",
    "diff_tex_t2.loc[:,\"Homogeneity\"] = np.abs(features_df.tex_homogeneity_t2_original - features_df.tex_homogeneity_t2_pred) / features_df.tex_homogeneity_t2_original * 100\n",
    "\n",
    "diff_tex_t2.loc[:,\"Contrast\"] = np.abs(features_df.tex_contrast_t2_original - features_df.tex_contrast_t2_pred) / features_df.tex_contrast_t2_original * 100\n",
    "\n",
    "diff_tex_t2.loc[:,\"Entropy\"] = np.abs(features_df.tex_entropy_t2_original - features_df.tex_entropy_t2_pred) / features_df.tex_entropy_t2_original * 100\n",
    "\n",
    "diff_tex_t2.loc[:,\"Correlation\"] = np.abs(features_df.tex_correlation_t2_original - features_df.tex_correlation_t2_pred) / features_df.tex_correlation_t2_original * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Energy</th>\n",
       "      <th>Homogeneity</th>\n",
       "      <th>Contrast</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Correlation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>283.970914</td>\n",
       "      <td>283.970914</td>\n",
       "      <td>283.970914</td>\n",
       "      <td>283.970914</td>\n",
       "      <td>283.970914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>718.948321</td>\n",
       "      <td>718.948321</td>\n",
       "      <td>718.948321</td>\n",
       "      <td>718.948321</td>\n",
       "      <td>718.948321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.937831</td>\n",
       "      <td>1.937831</td>\n",
       "      <td>1.937831</td>\n",
       "      <td>1.937831</td>\n",
       "      <td>1.937831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>14.228744</td>\n",
       "      <td>14.228744</td>\n",
       "      <td>14.228744</td>\n",
       "      <td>14.228744</td>\n",
       "      <td>14.228744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>25.428907</td>\n",
       "      <td>25.428907</td>\n",
       "      <td>25.428907</td>\n",
       "      <td>25.428907</td>\n",
       "      <td>25.428907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>103.732404</td>\n",
       "      <td>103.732404</td>\n",
       "      <td>103.732404</td>\n",
       "      <td>103.732404</td>\n",
       "      <td>103.732404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2712.248818</td>\n",
       "      <td>2712.248818</td>\n",
       "      <td>2712.248818</td>\n",
       "      <td>2712.248818</td>\n",
       "      <td>2712.248818</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Energy  Homogeneity     Contrast      Entropy  Correlation\n",
       "count    27.000000    27.000000    27.000000    27.000000    27.000000\n",
       "mean    283.970914   283.970914   283.970914   283.970914   283.970914\n",
       "std     718.948321   718.948321   718.948321   718.948321   718.948321\n",
       "min       1.937831     1.937831     1.937831     1.937831     1.937831\n",
       "25%      14.228744    14.228744    14.228744    14.228744    14.228744\n",
       "50%      25.428907    25.428907    25.428907    25.428907    25.428907\n",
       "75%     103.732404   103.732404   103.732404   103.732404   103.732404\n",
       "max    2712.248818  2712.248818  2712.248818  2712.248818  2712.248818"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_tex_t2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_tex_flair = pd.DataFrame()\n",
    "diff_tex_flair.loc[:,\"Energy\"] = np.abs(features_df.tex_energy_flair_original - features_df.tex_energy_flair_pred) / features_df.tex_energy_flair_original * 100\n",
    "\n",
    "\n",
    "diff_tex_flair.loc[:,\"Homogeneity\"] = np.abs(features_df.tex_homogeneity_flair_original - features_df.tex_homogeneity_flair_pred) / features_df.tex_homogeneity_flair_original * 100\n",
    "\n",
    "diff_tex_flair.loc[:,\"Contrast\"] = np.abs(features_df.tex_contrast_flair_original - features_df.tex_contrast_flair_pred) / features_df.tex_contrast_flair_original * 100\n",
    "\n",
    "diff_tex_flair.loc[:,\"Entropy\"] = np.abs(features_df.tex_entropy_flair_original - features_df.tex_entropy_flair_pred) / features_df.tex_entropy_flair_original * 100\n",
    "\n",
    "diff_tex_flair.loc[:,\"Correlation\"] = np.abs(features_df.tex_correlation_flair_original - features_df.tex_correlation_flair_pred) / features_df.tex_correlation_flair_original * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Energy</th>\n",
       "      <th>Homogeneity</th>\n",
       "      <th>Contrast</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Correlation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>289.124697</td>\n",
       "      <td>289.124697</td>\n",
       "      <td>289.124697</td>\n",
       "      <td>289.124697</td>\n",
       "      <td>289.124697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>727.874363</td>\n",
       "      <td>727.874363</td>\n",
       "      <td>727.874363</td>\n",
       "      <td>727.874363</td>\n",
       "      <td>727.874363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.942599</td>\n",
       "      <td>1.942599</td>\n",
       "      <td>1.942599</td>\n",
       "      <td>1.942599</td>\n",
       "      <td>1.942599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>17.441094</td>\n",
       "      <td>17.441094</td>\n",
       "      <td>17.441094</td>\n",
       "      <td>17.441094</td>\n",
       "      <td>17.441094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>25.299943</td>\n",
       "      <td>25.299943</td>\n",
       "      <td>25.299943</td>\n",
       "      <td>25.299943</td>\n",
       "      <td>25.299943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>110.075532</td>\n",
       "      <td>110.075532</td>\n",
       "      <td>110.075532</td>\n",
       "      <td>110.075532</td>\n",
       "      <td>110.075532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2814.091438</td>\n",
       "      <td>2814.091438</td>\n",
       "      <td>2814.091438</td>\n",
       "      <td>2814.091438</td>\n",
       "      <td>2814.091438</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Energy  Homogeneity     Contrast      Entropy  Correlation\n",
       "count    27.000000    27.000000    27.000000    27.000000    27.000000\n",
       "mean    289.124697   289.124697   289.124697   289.124697   289.124697\n",
       "std     727.874363   727.874363   727.874363   727.874363   727.874363\n",
       "min       1.942599     1.942599     1.942599     1.942599     1.942599\n",
       "25%      17.441094    17.441094    17.441094    17.441094    17.441094\n",
       "50%      25.299943    25.299943    25.299943    25.299943    25.299943\n",
       "75%     110.075532   110.075532   110.075532   110.075532   110.075532\n",
       "max    2814.091438  2814.091438  2814.091438  2814.091438  2814.091438"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_tex_flair.describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fyp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "37bd13aa3d924eb96db98b00aa626978b2fce1020e2653180f4604a647daf9d4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
