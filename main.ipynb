{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3D Segmentation of Brain Tumor\n",
    "\n",
    "This document is to test the methodology and refine it for execution in FYP B.\n",
    "\n",
    "\n",
    "## Loading the Dataset\n",
    "The dataset used is the BraTS 2020 Dataset.\n",
    "\n",
    "The Dataset Contains the Following Scans per case:\n",
    "\n",
    "T1: T1-weighted, native image, sagittal or axial 2D acquisitions, with 1–6 mm slice thickness.\n",
    "\n",
    "T1c: T1-weighted, contrast-enhanced (Gadolinium) image, with 3D acquisition and 1 mm isotropic voxel size for most patients.\n",
    "\n",
    "T2: T2-weighted image, axial 2D acquisition, with 2–6 mm slice thickness.\n",
    "\n",
    "FLAIR: T2-weighted FLAIR image, axial, coronal, or sagittal 2D acquisitions, 2–6 mm slice thickness.\n",
    "\n",
    "So for training T1ce, T2 and FLAIR will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for Google Colab Users\n",
    "%pip install monai==0.9.0\n",
    "%pip install monai[einops]\n",
    "%pip install pytorch_lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.data import Dataset, CacheDataset,PersistentDataset, decollate_batch\n",
    "from glob import glob\n",
    "from os import path, listdir\n",
    "import pytorch_lightning as pl\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "from monai.transforms import (\n",
    "    LoadImaged,\n",
    "    SpatialPadd,\n",
    "    EnsureChannelFirstd,\n",
    "    Compose,\n",
    "    NormalizeIntensityd,\n",
    "    Orientationd,\n",
    "    Spacingd,\n",
    "    RandSpatialCropd,\n",
    "    RandFlipd,\n",
    "    MapTransform,\n",
    "    CropForegroundd,\n",
    ")\n",
    "\n",
    "from monai.data import partition_dataset\n",
    "from monai.networks.nets import UNet,UNETR, SegResNet\n",
    "from monai.losses import TverskyLoss, DiceLoss\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.inferers import sliding_window_inference\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = r\"/content/drive/MyDrive/BraTS_2020/Training\"\n",
    "# Base Path to the training Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As there are 3 Main Classes to be identified:\n",
    "1. Non-Enhancing, $c_1$\n",
    "2. Edema, $c_2$\n",
    "3. [`Skipped`]\n",
    "4. Enhancing $c_3$\n",
    "5. No Tumor, Normal Tissue, $c_{none}$\n",
    "\n",
    "One-Hot Encoding is done on the segmented dataset to bring each Data point in the segmentation mask into an array similar to\n",
    "$$\n",
    " \\overbrace{(c_1 | c_2 | c_3 | c_{none})}^{\\text{A Segmented voxel}} \\rightarrow  \\begin{bmatrix}c_1\\\\c_2\\\\c_3\\end{bmatrix} \n",
    "$$\n",
    "\n",
    "where each point will correspond to a a class mentioned above. If all elements within the array are 0, then it means that the point is not classified as a malignancy (no tumor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Hot Encoding for Tumor as 3 is skipped in the segmentation masks. Moving 4 to 3.\n",
    "class ConvertLabelsIntoOneHotd(MapTransform):\n",
    "    def __call__(self, data):\n",
    "        data_dict = dict(data)\n",
    "        for key in self.keys:\n",
    "            one_hot_encode_array = [\n",
    "                data_dict[key] == 1,  # Non Enhancing Tumor Core\n",
    "                data_dict[key] == 2,  # Edema Core\n",
    "                data_dict[key] == 4,  # Enhancing Tumor Core\n",
    "            ]\n",
    "            data_dict[key] = np.stack(one_hot_encode_array, axis=0).astype(np.float32)\n",
    "        return data_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases = [path.join(base_path, x) for x in listdir(base_path)]\n",
    "# Appends all cases to a list\n",
    "\n",
    "mri_images = [\n",
    "    {\n",
    "        \"image\": [\n",
    "            path.join(case, f\"{path.split(case)[-1]}_t1ce.nii.gz\"),\n",
    "            path.join(case, f\"{path.split(case)[-1]}_t2.nii.gz\"),\n",
    "            path.join(case, f\"{path.split(case)[-1]}_flair.nii.gz\"),\n",
    "        ],\n",
    "        \"seg\": path.join(case, f\"{path.split(case)[-1]}_seg.nii.gz\"),\n",
    "    }\n",
    "    for case in cases[:30]  #! Only 30 cases taken as test slice\n",
    "]\n",
    "# Extract the path to the training nifti files of each case. Chosen are t1ce, t2 and flair. Segmentation is also extracted.\n",
    "\n",
    "training_data, validation_data = partition_dataset(mri_images, ratios=[0.7,0.3],shuffle=True)\n",
    "# 70 - 30 split of the training data.\n",
    "\n",
    "\n",
    "# TODO: Research more about the following transforms as well as importance of transforms for MRI data.\n",
    "transform_images_train = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"image\", \"seg\"]),  # Loads the images and segmentation\n",
    "        EnsureChannelFirstd(\n",
    "            keys=[\"image\"]\n",
    "        ),  # Ensures that the images are in the correct format, ie channel is first dimension.\n",
    "        ConvertLabelsIntoOneHotd(\n",
    "            keys=[\"seg\"]\n",
    "        ),  # Converts the segmentation into one hot encoding\n",
    "        RandSpatialCropd(keys=[\"image\", \"seg\"], roi_size=[224, 224, 144], random_size=False),\n",
    "        NormalizeIntensityd(\n",
    "            keys=\"image\", nonzero=True, channel_wise=True\n",
    "        ),  # Normalise Intensity of the images\n",
    "        Orientationd(keys=[\"image\", \"seg\"], axcodes=\"RAS\"),\n",
    "        Spacingd(\n",
    "            keys=[\"image\", \"seg\"],\n",
    "            pixdim=(1.0, 1.0, 1.0),\n",
    "            mode=(\"bilinear\", \"nearest\"),\n",
    "        ),  # Ensure spacing of voxels are uniform among scans\n",
    "        RandFlipd(keys=[\"image\", \"seg\"], prob=0.5, spatial_axis=0),\n",
    "        RandFlipd(keys=[\"image\", \"seg\"], prob=0.5, spatial_axis=1),\n",
    "        RandFlipd(\n",
    "            keys=[\"image\", \"seg\"], prob=0.5, spatial_axis=2\n",
    "        ),  # Three Random flips along the three axes\n",
    "    ]\n",
    ")\n",
    "# Defines the transformations to be applied to the training data.\n",
    "\n",
    "transform_images_validation = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"image\", \"seg\"]),  # Loads the images and segmentation\n",
    "        EnsureChannelFirstd(\n",
    "            keys=[\"image\"]\n",
    "        ),  # Ensures that the images are in the correct format, ie channel is first dimension.\n",
    "        ConvertLabelsIntoOneHotd(\n",
    "            keys=[\"seg\"]\n",
    "        ),  # Converts the segmentation into one hot encoding\n",
    "        NormalizeIntensityd(\n",
    "            keys=\"image\", nonzero=True, channel_wise=True\n",
    "        ),  # Normalise Intensity of the images\n",
    "        Orientationd(keys=[\"image\", \"seg\"], axcodes=\"RAS\"),\n",
    "        Spacingd(\n",
    "            keys=[\"image\", \"seg\"],\n",
    "            pixdim=(1.0, 1.0, 1.0),\n",
    "            mode=(\"bilinear\", \"nearest\"),\n",
    "        ),  # Ensure spacing of voxels are uniform among scans\n",
    "    ]\n",
    ")\n",
    "# Defines the transformations to be applied to the testing data.\n",
    "\n",
    "\n",
    "persistent_dataset_path = (\n",
    "    r\"./cache\"  # Path of the persistent dataset cache, to speed up training process\n",
    ")\n",
    "\n",
    "# dataset_training = Dataset(\n",
    "#     training_data, transform=transform_images_train\n",
    "# )  # Preparation of the training dataset <- Testing and debugging\n",
    "\n",
    "# dataset_validation = Dataset(\n",
    "#     validation_data, transform=transform_images_validation\n",
    "# )  # Preparation of the validation dataset <- Testing and debugging\n",
    "\n",
    "# dataset_training = PersistentDataset(\n",
    "#     mri_images, transform=transform_images_train, cache_dir=persistent_dataset_path\n",
    "# )  # Preparation of the training dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a Basic to Learn and test with limited Dataset\n",
    "class Model(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # self.model = UNet(\n",
    "        #     spatial_dims=3,\n",
    "        #     in_channels=3,\n",
    "        #     out_channels=3,\n",
    "        #     channels=(16, 32, 64, 128, 256),\n",
    "        #     strides=(2, 2, 2, 2),\n",
    "        # )\n",
    "\n",
    "        self.model = SegResNet(\n",
    "            blocks_down=[1, 2, 2, 3],\n",
    "            blocks_up=[1, 1, 1],\n",
    "            init_filters=16,\n",
    "            in_channels=3,\n",
    "            out_channels=3,\n",
    "            dropout_prob=0.2,\n",
    "        )\n",
    "\n",
    "        self.batch_size = 5  #! Adjust with tuning Later\n",
    "        self.num_workers = 2  #! Change for Machine / Temporarily omitted\n",
    "        self.learning_rate = 1e-3  #! Adjust with tuning Later\n",
    "\n",
    "        self.loss_function = DiceLoss(\n",
    "            to_onehot_y=False, sigmoid=True, squared_pred=True\n",
    "        )\n",
    "        self.dice_metric = DiceMetric(include_background=True, reduction=\"mean\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # Make the Datasets based on the sequence of data. To be passed into Data Loaders later.\n",
    "        self.train_ds = PersistentDataset(\n",
    "            training_data, transform=transform_images_train, cache_dir=\"./cache\"\n",
    "        )  # Preparation of the training dataset <- Testing and debugging\n",
    "\n",
    "        self.val_ds = PersistentDataset(\n",
    "            validation_data, transform=transform_images_validation, cache_dir=\"./cache\"\n",
    "        )  # Preparation of the validation dataset <- Testing and debugging\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.train_ds,\n",
    "            shuffle=True,\n",
    "            batch_size=self.batch_size,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.val_ds,\n",
    "            shuffle=True,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "        )\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        image, seg = batch[\"image\"], batch[\"seg\"]\n",
    "        output = self.forward(image)\n",
    "        loss = self.loss_function(output, seg)\n",
    "        tensorboard_logs = {\"train_loss\": loss.item()}\n",
    "        return {\"loss\": loss, \"log\": tensorboard_logs}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        image, seg = batch[\"image\"], batch[\"seg\"]\n",
    "        output = sliding_window_inference(\n",
    "            inputs=image,\n",
    "            roi_size=(240, 240, 160),\n",
    "            sw_batch_size=1,\n",
    "            predictor=self.forward,\n",
    "            overlap=0.5,\n",
    "        )\n",
    "        loss = self.loss_function(output, seg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaring Logger\n",
    "log_dir = r'/content/drive/MyDrive/BraTS_2020/logs'\n",
    "tb_logger = pl.loggers.TensorBoardLogger(\n",
    "    save_dir=log_dir\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `Trainer(progress_bar_refresh_rate=20)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "# Instantiating Model and tuning before training\n",
    "\n",
    "net = Model()\n",
    "trainer = pl.Trainer(\n",
    "    accelerator=\"gpu\",\n",
    "    gpus=1,\n",
    "    max_epochs=50,\n",
    "    auto_lr_find=True,\n",
    "    auto_scale_batch_size=True,\n",
    "    logger=tb_logger,\n",
    "    progress_bar_refresh_rate=20\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.tune(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Missing logger folder: ./logs\\lightning_logs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name          | Type     | Params\n",
      "-------------------------------------------\n",
      "0 | model         | UNet     | 2.0 M \n",
      "1 | loss_function | DiceLoss | 0     \n",
      "-------------------------------------------\n",
      "2.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.0 M     Total params\n",
      "7.925     Total estimated model params size (MB)\n",
      "The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9ad901a62114a87af928b8c71fb0288",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "def clear_cache_dir():\n",
    "    cwd = os.getcwd()\n",
    "    shutil.rmtree(os.path.join(cwd, 'cache'))\n",
    "    \n",
    "clear_cache_dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "# in case your GPU crashes and memory needs to be flushed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('mri')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ae52af71424788817ac7931c57470275392053099e15f5f167044ac7f80a887f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
