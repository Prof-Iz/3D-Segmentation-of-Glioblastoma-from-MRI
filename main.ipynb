{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3D Segmentation of Brain Tumor\n",
    "\n",
    "This document is to test the methodology and refine it for execution in FYP B.\n",
    "\n",
    "\n",
    "## Loading the Dataset\n",
    "The dataset used is the BraTS 2020 Dataset.\n",
    "\n",
    "The Dataset Contains the Following Scans per case:\n",
    "\n",
    "T1: T1-weighted, native image, sagittal or axial 2D acquisitions, with 1–6 mm slice thickness.\n",
    "\n",
    "T1c: T1-weighted, contrast-enhanced (Gadolinium) image, with 3D acquisition and 1 mm isotropic voxel size for most patients.\n",
    "\n",
    "T2: T2-weighted image, axial 2D acquisition, with 2–6 mm slice thickness.\n",
    "\n",
    "FLAIR: T2-weighted FLAIR image, axial, coronal, or sagittal 2D acquisitions, 2–6 mm slice thickness.\n",
    "\n",
    "So for training T1ce, T2 and FLAIR will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.data import Dataset, CacheDataset,PersistentDataset\n",
    "from glob import glob\n",
    "from os import path, listdir\n",
    "import pytorch_lightning as pl\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "from monai.transforms import (\n",
    "    LoadImaged,\n",
    "    SpatialPadd,\n",
    "    EnsureChannelFirstd,\n",
    "    Compose,\n",
    "    NormalizeIntensityd,\n",
    "    Orientationd,\n",
    "    Spacingd,\n",
    "    RandSpatialCropd,\n",
    "    RandFlipd,\n",
    "    MapTransform,\n",
    "    CropForegroundd,\n",
    ")\n",
    "\n",
    "from monai.data import partition_dataset\n",
    "from monai.networks.nets import UNet,UNETR, SegResNet\n",
    "from monai.losses import TverskyLoss, DiceLoss\n",
    "from monai.metrics import DiceMetric\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install monai==0.9.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = r\"Datasets\\2020\\MICCAI_BraTS2020_TrainingData\"\n",
    "# Base Path to the training Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As there are 3 Main Classes to be identified:\n",
    "1. Non-Enhancing, $c_1$\n",
    "2. Edema, $c_2$\n",
    "3. [`Skipped`]\n",
    "4. Enhancing $c_3$\n",
    "5. No Tumor, Normal Tissue, $c_{none}$\n",
    "\n",
    "One-Hot Encoding is done on the segmented dataset to bring each Data point in the segmentation mask into an array similar to\n",
    "$$\n",
    " \\overbrace{(c_1 | c_2 | c_3 | c_{none})}^{\\text{A Segmented voxel}} \\rightarrow  \\begin{bmatrix}c_1\\\\c_2\\\\c_3\\end{bmatrix} \n",
    "$$\n",
    "\n",
    "where each point will correspond to a a class mentioned above. If all elements within the array are 0, then it means that the point is not classified as a malignancy (no tumor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Hot Encoding for Tumor as 3 is skipped in the segmentation masks. Moving 4 to 3.\n",
    "class ConvertLabelsIntoOneHotd(MapTransform):\n",
    "    def __call__(self, data):\n",
    "        data_dict = dict(data)\n",
    "        for key in self.keys:\n",
    "            one_hot_encode_array = [\n",
    "                data_dict[key] == 1,  # Non Enhancing Tumor Core\n",
    "                data_dict[key] == 2,  # Edema Core\n",
    "                data_dict[key] == 4,  # Enhancing Tumor Core\n",
    "            ]\n",
    "            data_dict[key] = np.stack(one_hot_encode_array, axis=0).astype(np.float32)\n",
    "        return data_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases = [path.join(base_path, x) for x in listdir(base_path)]\n",
    "# Appends all cases to a list\n",
    "\n",
    "mri_images = [\n",
    "    {\n",
    "        \"image\": [\n",
    "            path.join(case, f\"{path.split(case)[-1]}_t1ce.nii.gz\"),\n",
    "            path.join(case, f\"{path.split(case)[-1]}_t2.nii.gz\"),\n",
    "            path.join(case, f\"{path.split(case)[-1]}_flair.nii.gz\"),\n",
    "        ],\n",
    "        \"seg\": path.join(case, f\"{path.split(case)[-1]}_seg.nii.gz\"),\n",
    "    }\n",
    "    for case in cases[:100]  #! Only 100 cases taken as test slice\n",
    "]\n",
    "# Extract the path to the training nifti files of each case. Chosen are t1ce, t2 and flair. Segmentation is also extracted.\n",
    "\n",
    "training_data, validation_data = partition_dataset(mri_images, ratios=[0.7,0.3],shuffle=True)\n",
    "# 70 - 30 split of the training data.\n",
    "\n",
    "\n",
    "# TODO: Research more about the following transforms as well as importance of transforms for MRI data.\n",
    "transform_images_train = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"image\", \"seg\"]),  # Loads the images and segmentation\n",
    "        EnsureChannelFirstd(\n",
    "            keys=[\"image\"]\n",
    "        ),  # Ensures that the images are in the correct format, ie channel is first dimension.\n",
    "        ConvertLabelsIntoOneHotd(\n",
    "            keys=[\"seg\"]\n",
    "        ),  # Converts the segmentation into one hot encoding\n",
    "        RandSpatialCropd(keys=[\"image\", \"seg\"], roi_size=[224, 224, 144], random_size=False),\n",
    "        NormalizeIntensityd(\n",
    "            keys=\"image\", nonzero=True, channel_wise=True\n",
    "        ),  # Normalise Intensity of the images\n",
    "        Orientationd(keys=[\"image\", \"seg\"], axcodes=\"RAS\"),\n",
    "        Spacingd(\n",
    "            keys=[\"image\", \"seg\"],\n",
    "            pixdim=(1.0, 1.0, 1.0),\n",
    "            mode=(\"bilinear\", \"nearest\"),\n",
    "        ),  # Ensure spacing of voxels are uniform among scans\n",
    "        RandFlipd(keys=[\"image\", \"seg\"], prob=0.5, spatial_axis=0),\n",
    "        RandFlipd(keys=[\"image\", \"seg\"], prob=0.5, spatial_axis=1),\n",
    "        RandFlipd(\n",
    "            keys=[\"image\", \"seg\"], prob=0.5, spatial_axis=2\n",
    "        ),  # Three Random flips along the three axes\n",
    "    ]\n",
    ")\n",
    "# Defines the transformations to be applied to the training data.\n",
    "\n",
    "transform_images_validation = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"image\", \"seg\"]),  # Loads the images and segmentation\n",
    "        EnsureChannelFirstd(\n",
    "            keys=[\"image\"]\n",
    "        ),  # Ensures that the images are in the correct format, ie channel is first dimension.\n",
    "        ConvertLabelsIntoOneHotd(\n",
    "            keys=[\"seg\"]\n",
    "        ),  # Converts the segmentation into one hot encoding\n",
    "        RandSpatialCropd(keys=[\"image\", \"seg\"], roi_size=[224, 224, 144], random_size=False),\n",
    "        NormalizeIntensityd(\n",
    "            keys=\"image\", nonzero=True, channel_wise=True\n",
    "        ),  # Normalise Intensity of the images\n",
    "        Orientationd(keys=[\"image\", \"seg\"], axcodes=\"RAS\"),\n",
    "        Spacingd(\n",
    "            keys=[\"image\", \"seg\"],\n",
    "            pixdim=(1.0, 1.0, 1.0),\n",
    "            mode=(\"bilinear\", \"nearest\"),\n",
    "        ),  # Ensure spacing of voxels are uniform among scans\n",
    "    ]\n",
    ")\n",
    "# Defines the transformations to be applied to the testing data.\n",
    "\n",
    "\n",
    "persistent_dataset_path = (\n",
    "    r\"./cache\"  # Path of the persistent dataset cache, to speed up training process\n",
    ")\n",
    "\n",
    "# dataset_training = Dataset(\n",
    "#     training_data, transform=transform_images_train\n",
    "# )  # Preparation of the training dataset <- Testing and debugging\n",
    "\n",
    "# dataset_validation = Dataset(\n",
    "#     validation_data, transform=transform_images_validation\n",
    "# )  # Preparation of the validation dataset <- Testing and debugging\n",
    "\n",
    "# dataset_training = PersistentDataset(\n",
    "#     mri_images, transform=transform_images_train, cache_dir=persistent_dataset_path\n",
    "# )  # Preparation of the training dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a Basic UNet to Learn and test with limited Dataset\n",
    "class Model(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = UNet(\n",
    "            spatial_dims=3,\n",
    "            in_channels=3,\n",
    "            out_channels=3,\n",
    "            channels=(16, 32, 64, 128, 256),\n",
    "            strides=(2, 2, 2, 2),\n",
    "        )\n",
    "\n",
    "        self.batch_size = 2  #! Adjust with tuning Later\n",
    "        self.num_workers = 4  #! Change for Machine / Temporarily omitted\n",
    "        self.learning_rate = 1e-4  #! Adjust with tuning Later\n",
    "\n",
    "        self.loss_function = DiceLoss(to_onehot_y=False, softmax=True)\n",
    "        self.dice_metric = DiceMetric(include_background=False, reduction=\"mean\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # Make the Datasets based on the sequence of data. To be passed into Data Loaders later.\n",
    "        self.train_ds = PersistentDataset(\n",
    "            training_data, transform=transform_images_train, cache_dir=\"./cache\"\n",
    "        )  # Preparation of the training dataset <- Testing and debugging\n",
    "\n",
    "        self.val_ds = PersistentDataset(\n",
    "            validation_data, transform=transform_images_validation, cache_dir=\"./cache\"\n",
    "        )  # Preparation of the validation dataset <- Testing and debugging\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.train_ds,\n",
    "            shuffle=True,\n",
    "            batch_size=self.batch_size,\n",
    "        )\n",
    "\n",
    "    # def val_dataloader(self):\n",
    "    #     return torch.utils.data.DataLoader(\n",
    "    #         self.val_ds,\n",
    "    #         shuffle=True,\n",
    "    #         batch_size=self.batch_size,\n",
    "    #         num_workers=self.num_workers,\n",
    "    #     )\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        image, seg = batch[\"image\"], batch[\"seg\"]\n",
    "        output = self.forward(image)\n",
    "        loss = self.loss_function(output, seg)\n",
    "        return {\"loss\": loss}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaring Logger\n",
    "log_dir = r'./logs'\n",
    "tb_logger = pl.loggers.TensorBoardLogger(\n",
    "    save_dir=log_dir\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "# Instantiating Model and tuning before training\n",
    "\n",
    "net = Model()\n",
    "trainer = pl.Trainer(\n",
    "    accelerator=\"gpu\",\n",
    "    gpus=1,\n",
    "    max_epochs=5,\n",
    "    auto_lr_find=True,\n",
    "    auto_scale_batch_size=True,\n",
    "    move_metrics_to_cpu=True,\n",
    "    logger=tb_logger,\n",
    "    progress_bar_refresh_rate=20\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\University\\OneDrive - UCSI University\\FYP 2022\\Code\\3D-Segmentation-of-Glioblastoma-from-MRI\\main.ipynb Cell 10'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/University/OneDrive%20-%20UCSI%20University/FYP%202022/Code/3D-Segmentation-of-Glioblastoma-from-MRI/main.ipynb#ch0000016?line=0'>1</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtune(net)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\mri\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1128\u001b[0m, in \u001b[0;36mTrainer.tune\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, scale_batch_size_kwargs, lr_find_kwargs)\u001b[0m\n\u001b[0;32m   1123\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_connector\u001b[39m.\u001b[39mattach_data(\n\u001b[0;32m   1124\u001b[0m     model, train_dataloaders\u001b[39m=\u001b[39mtrain_dataloaders, val_dataloaders\u001b[39m=\u001b[39mval_dataloaders, datamodule\u001b[39m=\u001b[39mdatamodule\n\u001b[0;32m   1125\u001b[0m )\n\u001b[0;32m   1127\u001b[0m \u001b[39mwith\u001b[39;00m isolate_rng():\n\u001b[1;32m-> 1128\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtuner\u001b[39m.\u001b[39;49m_tune(\n\u001b[0;32m   1129\u001b[0m         model, scale_batch_size_kwargs\u001b[39m=\u001b[39;49mscale_batch_size_kwargs, lr_find_kwargs\u001b[39m=\u001b[39;49mlr_find_kwargs\n\u001b[0;32m   1130\u001b[0m     )\n\u001b[0;32m   1132\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[0;32m   1133\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtuning \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\mri\\lib\\site-packages\\pytorch_lightning\\tuner\\tuning.py:58\u001b[0m, in \u001b[0;36mTuner._tune\u001b[1;34m(self, model, scale_batch_size_kwargs, lr_find_kwargs)\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mauto_scale_batch_size, \u001b[39mstr\u001b[39m):\n\u001b[0;32m     57\u001b[0m         scale_batch_size_kwargs\u001b[39m.\u001b[39msetdefault(\u001b[39m\"\u001b[39m\u001b[39mmode\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mauto_scale_batch_size)\n\u001b[1;32m---> 58\u001b[0m     result[\u001b[39m\"\u001b[39m\u001b[39mscale_batch_size\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m scale_batch_size(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer, model, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mscale_batch_size_kwargs)\n\u001b[0;32m     60\u001b[0m \u001b[39m# Run learning rate finder:\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mauto_lr_find:\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\mri\\lib\\site-packages\\pytorch_lightning\\tuner\\batch_size_scaling.py:75\u001b[0m, in \u001b[0;36mscale_batch_size\u001b[1;34m(trainer, model, mode, steps_per_trial, init_val, max_trials, batch_arg_name)\u001b[0m\n\u001b[0;32m     73\u001b[0m new_size, _ \u001b[39m=\u001b[39m _adjust_batch_size(trainer, batch_arg_name, value\u001b[39m=\u001b[39minit_val)  \u001b[39m# initially set to init_val\u001b[39;00m\n\u001b[0;32m     74\u001b[0m \u001b[39mif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpower\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m---> 75\u001b[0m     new_size \u001b[39m=\u001b[39m _run_power_scaling(trainer, model, new_size, batch_arg_name, max_trials)\n\u001b[0;32m     76\u001b[0m \u001b[39melif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbinsearch\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m     77\u001b[0m     new_size \u001b[39m=\u001b[39m _run_binsearch_scaling(trainer, model, new_size, batch_arg_name, max_trials)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\mri\\lib\\site-packages\\pytorch_lightning\\tuner\\batch_size_scaling.py:134\u001b[0m, in \u001b[0;36m_run_power_scaling\u001b[1;34m(trainer, model, new_size, batch_arg_name, max_trials)\u001b[0m\n\u001b[0;32m    131\u001b[0m trainer\u001b[39m.\u001b[39mfit_loop\u001b[39m.\u001b[39mglobal_step \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m  \u001b[39m# reset after each try\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    133\u001b[0m     \u001b[39m# Try fit\u001b[39;00m\n\u001b[1;32m--> 134\u001b[0m     trainer\u001b[39m.\u001b[39;49mtuner\u001b[39m.\u001b[39;49m_run(model)\n\u001b[0;32m    135\u001b[0m     \u001b[39m# Double in size\u001b[39;00m\n\u001b[0;32m    136\u001b[0m     new_size, changed \u001b[39m=\u001b[39m _adjust_batch_size(trainer, batch_arg_name, factor\u001b[39m=\u001b[39m\u001b[39m2.0\u001b[39m, desc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msucceeded\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\mri\\lib\\site-packages\\pytorch_lightning\\tuner\\tuning.py:73\u001b[0m, in \u001b[0;36mTuner._run\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstatus \u001b[39m=\u001b[39m TrainerStatus\u001b[39m.\u001b[39mRUNNING  \u001b[39m# last `_run` call might have set it to `FINISHED`\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39m_run(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     74\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mtuning \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\mri\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1236\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m   1232\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mrestore_training_state()\n\u001b[0;32m   1234\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mresume_end()\n\u001b[1;32m-> 1236\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[0;32m   1238\u001b[0m log\u001b[39m.\u001b[39mdetail(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1239\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_teardown()\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\mri\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1323\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1321\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredicting:\n\u001b[0;32m   1322\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_predict()\n\u001b[1;32m-> 1323\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_train()\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\mri\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1353\u001b[0m, in \u001b[0;36mTrainer._run_train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1351\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit_loop\u001b[39m.\u001b[39mtrainer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\n\u001b[0;32m   1352\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mset_detect_anomaly(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_detect_anomaly):\n\u001b[1;32m-> 1353\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_loop\u001b[39m.\u001b[39;49mrun()\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\mri\\lib\\site-packages\\pytorch_lightning\\loops\\base.py:204\u001b[0m, in \u001b[0;36mLoop.run\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    203\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 204\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madvance(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    205\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[0;32m    206\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\mri\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:269\u001b[0m, in \u001b[0;36mFitLoop.advance\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    265\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_fetcher\u001b[39m.\u001b[39msetup(\n\u001b[0;32m    266\u001b[0m     dataloader, batch_to_device\u001b[39m=\u001b[39mpartial(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39m_call_strategy_hook, \u001b[39m\"\u001b[39m\u001b[39mbatch_to_device\u001b[39m\u001b[39m\"\u001b[39m, dataloader_idx\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m    267\u001b[0m )\n\u001b[0;32m    268\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39m\"\u001b[39m\u001b[39mrun_training_epoch\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 269\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepoch_loop\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_fetcher)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\mri\\lib\\site-packages\\pytorch_lightning\\loops\\base.py:199\u001b[0m, in \u001b[0;36mLoop.run\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_skip()\n\u001b[0;32m    197\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreset()\n\u001b[1;32m--> 199\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_run_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    201\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdone:\n\u001b[0;32m    202\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\mri\\lib\\site-packages\\pytorch_lightning\\loops\\epoch\\training_epoch_loop.py:153\u001b[0m, in \u001b[0;36mTrainingEpochLoop.on_run_start\u001b[1;34m(self, data_fetcher)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mon_run_start\u001b[39m(\u001b[39mself\u001b[39m, data_fetcher: AbstractDataFetcher) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:  \u001b[39m# type: ignore[override]\u001b[39;00m\n\u001b[0;32m    152\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reload_dataloader_state_dict(data_fetcher)\n\u001b[1;32m--> 153\u001b[0m     _ \u001b[39m=\u001b[39m \u001b[39miter\u001b[39;49m(data_fetcher)  \u001b[39m# creates the iterator inside the fetcher\u001b[39;00m\n\u001b[0;32m    154\u001b[0m     \u001b[39m# add the previous `fetched` value to properly track `is_last_batch` with no prefetching\u001b[39;00m\n\u001b[0;32m    155\u001b[0m     data_fetcher\u001b[39m.\u001b[39mfetched \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mcurrent\u001b[39m.\u001b[39mready\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\mri\\lib\\site-packages\\pytorch_lightning\\utilities\\fetching.py:179\u001b[0m, in \u001b[0;36mAbstractDataFetcher.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreset()\n\u001b[0;32m    178\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataloader_iter \u001b[39m=\u001b[39m \u001b[39miter\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataloader)\n\u001b[1;32m--> 179\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply_patch()\n\u001b[0;32m    180\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprefetching()\n\u001b[0;32m    181\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\mri\\lib\\site-packages\\pytorch_lightning\\utilities\\fetching.py:115\u001b[0m, in \u001b[0;36mAbstractDataFetcher._apply_patch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    112\u001b[0m         loader\u001b[39m.\u001b[39m_lightning_fetcher \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\n\u001b[0;32m    113\u001b[0m         patch_dataloader_iterator(loader, iterator, \u001b[39mself\u001b[39m)\n\u001b[1;32m--> 115\u001b[0m apply_to_collections(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloaders, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloader_iters, (Iterator, DataLoader), _apply_patch_fn)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\mri\\lib\\site-packages\\pytorch_lightning\\utilities\\fetching.py:154\u001b[0m, in \u001b[0;36mAbstractDataFetcher.loader_iters\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[39mraise\u001b[39;00m MisconfigurationException(\u001b[39m\"\u001b[39m\u001b[39mThe `dataloader_iter` isn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt available outside the __iter__ context.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    153\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataloader, CombinedLoader):\n\u001b[1;32m--> 154\u001b[0m     loader_iters \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataloader_iter\u001b[39m.\u001b[39;49mloader_iters\n\u001b[0;32m    155\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    156\u001b[0m     loader_iters \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataloader_iter]\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\mri\\lib\\site-packages\\pytorch_lightning\\trainer\\supporters.py:540\u001b[0m, in \u001b[0;36mCombinedLoaderIterator.loader_iters\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    538\u001b[0m \u001b[39m\"\"\"Get the `_loader_iters` and create one if it is None.\"\"\"\u001b[39;00m\n\u001b[0;32m    539\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_loader_iters \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 540\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_loader_iters \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcreate_loader_iters(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloaders)\n\u001b[0;32m    542\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_loader_iters\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\mri\\lib\\site-packages\\pytorch_lightning\\trainer\\supporters.py:580\u001b[0m, in \u001b[0;36mCombinedLoaderIterator.create_loader_iters\u001b[1;34m(loaders)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[39m\"\"\"Create and return a collection of iterators from loaders.\u001b[39;00m\n\u001b[0;32m    572\u001b[0m \n\u001b[0;32m    573\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    577\u001b[0m \u001b[39m    a collections of iterators\u001b[39;00m\n\u001b[0;32m    578\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    579\u001b[0m \u001b[39m# dataloaders are Iterable but not Sequences. Need this to specifically exclude sequences\u001b[39;00m\n\u001b[1;32m--> 580\u001b[0m \u001b[39mreturn\u001b[39;00m apply_to_collection(loaders, Iterable, \u001b[39miter\u001b[39;49m, wrong_dtype\u001b[39m=\u001b[39;49m(Sequence, Mapping))\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\mri\\lib\\site-packages\\pytorch_lightning\\utilities\\apply_func.py:99\u001b[0m, in \u001b[0;36mapply_to_collection\u001b[1;34m(data, dtype, function, wrong_dtype, include_none, *args, **kwargs)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[39m# Breaking condition\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, dtype) \u001b[39mand\u001b[39;00m (wrong_dtype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(data, wrong_dtype)):\n\u001b[1;32m---> 99\u001b[0m     \u001b[39mreturn\u001b[39;00m function(data, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    101\u001b[0m elem_type \u001b[39m=\u001b[39m \u001b[39mtype\u001b[39m(data)\n\u001b[0;32m    103\u001b[0m \u001b[39m# Recursively apply to collection items\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\mri\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:368\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    366\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterator\n\u001b[0;32m    367\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 368\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_iterator()\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\mri\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:311\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    309\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_iterator\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m'\u001b[39m\u001b[39m_BaseDataLoaderIter\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    310\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_workers \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m--> 311\u001b[0m         \u001b[39mreturn\u001b[39;00m _SingleProcessDataLoaderIter(\u001b[39mself\u001b[39;49m)\n\u001b[0;32m    312\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    313\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_worker_number_rationality()\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\mri\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:561\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter.__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m    560\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, loader):\n\u001b[1;32m--> 561\u001b[0m     \u001b[39msuper\u001b[39;49m(_SingleProcessDataLoaderIter, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(loader)\n\u001b[0;32m    562\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m    563\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_workers \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\mri\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:507\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m    505\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_collate_fn \u001b[39m=\u001b[39m loader\u001b[39m.\u001b[39mcollate_fn\n\u001b[0;32m    506\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39m=\u001b[39m \u001b[39miter\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_index_sampler)\n\u001b[1;32m--> 507\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_base_seed \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mempty((), dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mint64)\u001b[39m.\u001b[39;49mrandom_(generator\u001b[39m=\u001b[39;49mloader\u001b[39m.\u001b[39;49mgenerator)\u001b[39m.\u001b[39;49mitem()\n\u001b[0;32m    508\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_persistent_workers \u001b[39m=\u001b[39m loader\u001b[39m.\u001b[39mpersistent_workers\n\u001b[0;32m    509\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.tune(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "def clear_cache_dir():\n",
    "    cwd = os.getcwd()\n",
    "    shutil.rmtree(os.path.join(cwd, 'cache'))\n",
    "    \n",
    "clear_cache_dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('mri')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ae52af71424788817ac7931c57470275392053099e15f5f167044ac7f80a887f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
