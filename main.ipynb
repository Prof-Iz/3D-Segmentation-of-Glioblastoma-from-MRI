{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3D Segmentation of Brain Tumor\n",
    "\n",
    "This document is to test the methodology and refine it for execution in FYP B.\n",
    "\n",
    "\n",
    "## Loading the Dataset\n",
    "The dataset used is the BraTS 2020 Dataset.\n",
    "\n",
    "The Dataset Contains the Following Scans per case:\n",
    "\n",
    "T1: T1-weighted, native image, sagittal or axial 2D acquisitions, with 1–6 mm slice thickness.\n",
    "\n",
    "T1c: T1-weighted, contrast-enhanced (Gadolinium) image, with 3D acquisition and 1 mm isotropic voxel size for most patients.\n",
    "\n",
    "T2: T2-weighted image, axial 2D acquisition, with 2–6 mm slice thickness.\n",
    "\n",
    "FLAIR: T2-weighted FLAIR image, axial, coronal, or sagittal 2D acquisitions, 2–6 mm slice thickness.\n",
    "\n",
    "So for training T1ce, T2 and FLAIR will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.data import Dataset, CacheDataset,PersistentDataset\n",
    "from glob import glob\n",
    "from os import path, listdir\n",
    "import pytorch_lightning as pl\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "from monai.transforms import (\n",
    "    LoadImaged,\n",
    "    SpatialPadd,\n",
    "    EnsureChannelFirstd,\n",
    "    Compose,\n",
    "    NormalizeIntensityd,\n",
    "    Orientationd,\n",
    "    Spacingd,\n",
    "    RandSpatialCropd,\n",
    "    RandFlipd,\n",
    "    MapTransform,\n",
    "    CropForegroundd,\n",
    ")\n",
    "\n",
    "from monai.data import partition_dataset\n",
    "from monai.networks.nets import UNet,UNETR, SegResNet\n",
    "from monai.losses import TverskyLoss, DiceLoss\n",
    "from monai.metrics import DiceMetric\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: monai==0.9.0 in c:\\users\\user\\anaconda3\\envs\\mri\\lib\\site-packages (0.9.0)\n",
      "Requirement already satisfied: torch>=1.7 in c:\\users\\user\\anaconda3\\envs\\mri\\lib\\site-packages (from monai==0.9.0) (1.11.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\user\\anaconda3\\envs\\mri\\lib\\site-packages (from monai==0.9.0) (1.22.3)\n",
      "Requirement already satisfied: typing_extensions in c:\\users\\user\\anaconda3\\envs\\mri\\lib\\site-packages (from torch>=1.7->monai==0.9.0) (4.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install monai==0.9.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = r\"Datasets\\2020\\MICCAI_BraTS2020_TrainingData\"\n",
    "# Base Path to the training Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As there are 3 Main Classes to be identified:\n",
    "1. Non-Enhancing, $c_1$\n",
    "2. Edema, $c_2$\n",
    "3. [`Skipped`]\n",
    "4. Enhancing $c_3$\n",
    "5. No Tumor, Normal Tissue, $c_{none}$\n",
    "\n",
    "One-Hot Encoding is done on the segmented dataset to bring each Data point in the segmentation mask into an array similar to\n",
    "$$\n",
    " \\overbrace{(c_1 | c_2 | c_3 | c_{none})}^{\\text{A Segmented voxel}} \\rightarrow  \\begin{bmatrix}c_1\\\\c_2\\\\c_3\\end{bmatrix} \n",
    "$$\n",
    "\n",
    "where each point will correspond to a a class mentioned above. If all elements within the array are 0, then it means that the point is not classified as a malignancy (no tumor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Hot Encoding for Tumor as 3 is skipped in the segmentation masks. Moving 4 to 3.\n",
    "class ConvertLabelsIntoOneHotd(MapTransform):\n",
    "    def __call__(self, data):\n",
    "        data_dict = dict(data)\n",
    "        for key in self.keys:\n",
    "            one_hot_encode_array = [\n",
    "                data_dict[key] == 1,  # Non Enhancing Tumor Core\n",
    "                data_dict[key] == 2,  # Edema Core\n",
    "                data_dict[key] == 4,  # Enhancing Tumor Core\n",
    "            ]\n",
    "            data_dict[key] = np.stack(one_hot_encode_array, axis=0).astype(np.float32)\n",
    "        return data_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases = [path.join(base_path, x) for x in listdir(base_path)]\n",
    "# Appends all cases to a list\n",
    "\n",
    "mri_images = [\n",
    "    {\n",
    "        \"image\": [\n",
    "            path.join(case, f\"{path.split(case)[-1]}_t1ce.nii.gz\"),\n",
    "            path.join(case, f\"{path.split(case)[-1]}_t2.nii.gz\"),\n",
    "            path.join(case, f\"{path.split(case)[-1]}_flair.nii.gz\"),\n",
    "        ],\n",
    "        \"seg\": path.join(case, f\"{path.split(case)[-1]}_seg.nii.gz\"),\n",
    "    }\n",
    "    for case in cases[:100]  #! Only 100 cases taken as test slice\n",
    "]\n",
    "# Extract the path to the training nifti files of each case. Chosen are t1ce, t2 and flair. Segmentation is also extracted.\n",
    "\n",
    "training_data, validation_data = partition_dataset(mri_images, ratios=[0.7,0.3],shuffle=True)\n",
    "# 70 - 30 split of the training data.\n",
    "\n",
    "\n",
    "# TODO: Research more about the following transforms as well as importance of transforms for MRI data.\n",
    "transform_images_train = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"image\", \"seg\"]),  # Loads the images and segmentation\n",
    "        EnsureChannelFirstd(\n",
    "            keys=[\"image\"]\n",
    "        ),  # Ensures that the images are in the correct format, ie channel is first dimension.\n",
    "        ConvertLabelsIntoOneHotd(\n",
    "            keys=[\"seg\"]\n",
    "        ),  # Converts the segmentation into one hot encoding\n",
    "        RandSpatialCropd(keys=[\"image\", \"seg\"], roi_size=[224, 224, 144], random_size=False),\n",
    "        NormalizeIntensityd(\n",
    "            keys=\"image\", nonzero=True, channel_wise=True\n",
    "        ),  # Normalise Intensity of the images\n",
    "        Orientationd(keys=[\"image\", \"seg\"], axcodes=\"RAS\"),\n",
    "        Spacingd(\n",
    "            keys=[\"image\", \"seg\"],\n",
    "            pixdim=(1.0, 1.0, 1.0),\n",
    "            mode=(\"bilinear\", \"nearest\"),\n",
    "        ),  # Ensure spacing of voxels are uniform among scans\n",
    "        RandFlipd(keys=[\"image\", \"seg\"], prob=0.5, spatial_axis=0),\n",
    "        RandFlipd(keys=[\"image\", \"seg\"], prob=0.5, spatial_axis=1),\n",
    "        RandFlipd(\n",
    "            keys=[\"image\", \"seg\"], prob=0.5, spatial_axis=2\n",
    "        ),  # Three Random flips along the three axes\n",
    "    ]\n",
    ")\n",
    "# Defines the transformations to be applied to the training data.\n",
    "\n",
    "transform_images_validation = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"image\", \"seg\"]),  # Loads the images and segmentation\n",
    "        EnsureChannelFirstd(\n",
    "            keys=[\"image\"]\n",
    "        ),  # Ensures that the images are in the correct format, ie channel is first dimension.\n",
    "        ConvertLabelsIntoOneHotd(\n",
    "            keys=[\"seg\"]\n",
    "        ),  # Converts the segmentation into one hot encoding\n",
    "        RandSpatialCropd(keys=[\"image\", \"seg\"], roi_size=[224, 224, 144], random_size=False),\n",
    "        NormalizeIntensityd(\n",
    "            keys=\"image\", nonzero=True, channel_wise=True\n",
    "        ),  # Normalise Intensity of the images\n",
    "        Orientationd(keys=[\"image\", \"seg\"], axcodes=\"RAS\"),\n",
    "        Spacingd(\n",
    "            keys=[\"image\", \"seg\"],\n",
    "            pixdim=(1.0, 1.0, 1.0),\n",
    "            mode=(\"bilinear\", \"nearest\"),\n",
    "        ),  # Ensure spacing of voxels are uniform among scans\n",
    "    ]\n",
    ")\n",
    "# Defines the transformations to be applied to the testing data.\n",
    "\n",
    "\n",
    "persistent_dataset_path = (\n",
    "    r\"./cache\"  # Path of the persistent dataset cache, to speed up training process\n",
    ")\n",
    "\n",
    "dataset_training = Dataset(\n",
    "    training_data, transform=transform_images_train\n",
    ")  # Preparation of the training dataset <- Testing and debugging\n",
    "\n",
    "dataset_validation = Dataset(\n",
    "    validation_data, transform=transform_images_validation\n",
    ")  # Preparation of the validation dataset <- Testing and debugging\n",
    "\n",
    "# dataset_training = PersistentDataset(\n",
    "#     mri_images, transform=transform_images_train, cache_dir=persistent_dataset_path\n",
    "# )  # Preparation of the training dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.5"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "144/32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x23fc8240a90>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(dataset_training[0]['seg'][0][:,100,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a Basic UNet to Learn and test with limited Dataset\n",
    "class Model(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = UNet(\n",
    "            spatial_dims=3,\n",
    "            in_channels=3,\n",
    "            out_channels=3,\n",
    "            channels=(16, 32, 64, 128, 256),\n",
    "            strides=(2, 2, 2, 2),\n",
    "        )\n",
    "\n",
    "        self.batch_size = 2  #! Adjust with tuning Later\n",
    "        self.num_workers = 4  #! Change for Machine / Temporarily omitted\n",
    "        self.learning_rate = 1e-4  #! Adjust with tuning Later\n",
    "\n",
    "        self.loss_function = DiceLoss(to_onehot_y=False, softmax=True)\n",
    "        self.dice_metric = DiceMetric(include_background=False, reduction=\"mean\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # Make the Datasets based on the sequence of data. To be passed into Data Loaders later.\n",
    "        self.train_ds = PersistentDataset(\n",
    "            training_data, transform=transform_images_train, cache_dir=\"./cache\"\n",
    "        )  # Preparation of the training dataset <- Testing and debugging\n",
    "\n",
    "        self.val_ds = PersistentDataset(\n",
    "            validation_data, transform=transform_images_validation, cache_dir=\"./cache\"\n",
    "        )  # Preparation of the validation dataset <- Testing and debugging\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.train_ds,\n",
    "            shuffle=True,\n",
    "            batch_size=self.batch_size,\n",
    "        )\n",
    "\n",
    "    # def val_dataloader(self):\n",
    "    #     return torch.utils.data.DataLoader(\n",
    "    #         self.val_ds,\n",
    "    #         shuffle=True,\n",
    "    #         batch_size=self.batch_size,\n",
    "    #         num_workers=self.num_workers,\n",
    "    #     )\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        image, seg = batch[\"image\"], batch[\"seg\"]\n",
    "        output = self.forward(image)\n",
    "        loss = self.loss_function(output, seg)\n",
    "        return {\"loss\": loss}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "# Instantiating Model and tuning before training\n",
    "\n",
    "net = Model()\n",
    "trainer = pl.Trainer(\n",
    "    accelerator=\"gpu\",\n",
    "    gpus=1,\n",
    "    max_epochs=5,\n",
    "    auto_lr_find=True,\n",
    "    auto_scale_batch_size=True,\n",
    "    move_metrics_to_cpu=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "The number of training batches (35) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "Batch size 2 succeeded, trying batch size 4\n",
      "The number of training batches (18) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Batch size 4 succeeded, trying batch size 8\n",
      "The number of training batches (9) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Batch size 8 succeeded, trying batch size 16\n",
      "The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Batch size 16 succeeded, trying batch size 32\n",
      "The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Batch size 32 succeeded, trying batch size 64\n",
      "The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Batch size 64 succeeded, trying batch size 128\n",
      "The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Batch size 70 succeeded, trying batch size 140\n",
      "Finished batch size finder, will continue with full run using batch size 70\n",
      "Restoring states from the checkpoint path at d:\\University\\OneDrive - UCSI University\\FYP 2022\\Code\\3D-Segmentation-of-Glioblastoma-from-MRI\\.scale_batch_size_63c7d3f1-06b6-4950-b9e0-bbce9e90a942.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\University\\OneDrive - UCSI University\\FYP 2022\\Code\\3D-Segmentation-of-Glioblastoma-from-MRI\\main.ipynb Cell 12'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/University/OneDrive%20-%20UCSI%20University/FYP%202022/Code/3D-Segmentation-of-Glioblastoma-from-MRI/main.ipynb#ch0000016?line=0'>1</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtune(net)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\mri\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1128\u001b[0m, in \u001b[0;36mTrainer.tune\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, scale_batch_size_kwargs, lr_find_kwargs)\u001b[0m\n\u001b[0;32m   1123\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_connector\u001b[39m.\u001b[39mattach_data(\n\u001b[0;32m   1124\u001b[0m     model, train_dataloaders\u001b[39m=\u001b[39mtrain_dataloaders, val_dataloaders\u001b[39m=\u001b[39mval_dataloaders, datamodule\u001b[39m=\u001b[39mdatamodule\n\u001b[0;32m   1125\u001b[0m )\n\u001b[0;32m   1127\u001b[0m \u001b[39mwith\u001b[39;00m isolate_rng():\n\u001b[1;32m-> 1128\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtuner\u001b[39m.\u001b[39;49m_tune(\n\u001b[0;32m   1129\u001b[0m         model, scale_batch_size_kwargs\u001b[39m=\u001b[39;49mscale_batch_size_kwargs, lr_find_kwargs\u001b[39m=\u001b[39;49mlr_find_kwargs\n\u001b[0;32m   1130\u001b[0m     )\n\u001b[0;32m   1132\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[0;32m   1133\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtuning \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\mri\\lib\\site-packages\\pytorch_lightning\\tuner\\tuning.py:63\u001b[0m, in \u001b[0;36mTuner._tune\u001b[1;34m(self, model, scale_batch_size_kwargs, lr_find_kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mauto_lr_find:\n\u001b[0;32m     62\u001b[0m     lr_find_kwargs\u001b[39m.\u001b[39msetdefault(\u001b[39m\"\u001b[39m\u001b[39mupdate_attr\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m---> 63\u001b[0m     result[\u001b[39m\"\u001b[39m\u001b[39mlr_find\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m lr_find(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer, model, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mlr_find_kwargs)\n\u001b[0;32m     65\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstatus \u001b[39m=\u001b[39m TrainerStatus\u001b[39m.\u001b[39mFINISHED\n\u001b[0;32m     67\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\mri\\lib\\site-packages\\pytorch_lightning\\tuner\\lr_finder.py:224\u001b[0m, in \u001b[0;36mlr_find\u001b[1;34m(trainer, model, min_lr, max_lr, num_training, mode, early_stop_threshold, update_attr)\u001b[0m\n\u001b[0;32m    221\u001b[0m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39msetup_optimizers \u001b[39m=\u001b[39m lr_finder\u001b[39m.\u001b[39m_exchange_scheduler(trainer, model)\n\u001b[0;32m    223\u001b[0m \u001b[39m# Fit, lr & loss logged in callback\u001b[39;00m\n\u001b[1;32m--> 224\u001b[0m trainer\u001b[39m.\u001b[39;49mtuner\u001b[39m.\u001b[39;49m_run(model)\n\u001b[0;32m    226\u001b[0m \u001b[39m# Prompt if we stopped early\u001b[39;00m\n\u001b[0;32m    227\u001b[0m \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mglobal_step \u001b[39m!=\u001b[39m num_training:\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\mri\\lib\\site-packages\\pytorch_lightning\\tuner\\tuning.py:73\u001b[0m, in \u001b[0;36mTuner._run\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstatus \u001b[39m=\u001b[39m TrainerStatus\u001b[39m.\u001b[39mRUNNING  \u001b[39m# last `_run` call might have set it to `FINISHED`\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39m_run(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     74\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mtuning \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\mri\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1236\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m   1232\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mrestore_training_state()\n\u001b[0;32m   1234\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mresume_end()\n\u001b[1;32m-> 1236\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[0;32m   1238\u001b[0m log\u001b[39m.\u001b[39mdetail(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1239\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_teardown()\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\mri\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1323\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1321\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredicting:\n\u001b[0;32m   1322\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_predict()\n\u001b[1;32m-> 1323\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_train()\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\mri\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1353\u001b[0m, in \u001b[0;36mTrainer._run_train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1351\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit_loop\u001b[39m.\u001b[39mtrainer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\n\u001b[0;32m   1352\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mset_detect_anomaly(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_detect_anomaly):\n\u001b[1;32m-> 1353\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_loop\u001b[39m.\u001b[39;49mrun()\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\mri\\lib\\site-packages\\pytorch_lightning\\loops\\base.py:204\u001b[0m, in \u001b[0;36mLoop.run\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    203\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 204\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madvance(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    205\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[0;32m    206\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\mri\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:269\u001b[0m, in \u001b[0;36mFitLoop.advance\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    265\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_fetcher\u001b[39m.\u001b[39msetup(\n\u001b[0;32m    266\u001b[0m     dataloader, batch_to_device\u001b[39m=\u001b[39mpartial(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39m_call_strategy_hook, \u001b[39m\"\u001b[39m\u001b[39mbatch_to_device\u001b[39m\u001b[39m\"\u001b[39m, dataloader_idx\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m    267\u001b[0m )\n\u001b[0;32m    268\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39m\"\u001b[39m\u001b[39mrun_training_epoch\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 269\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepoch_loop\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_fetcher)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\mri\\lib\\site-packages\\pytorch_lightning\\loops\\base.py:204\u001b[0m, in \u001b[0;36mLoop.run\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    203\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 204\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madvance(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    205\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[0;32m    206\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\mri\\lib\\site-packages\\pytorch_lightning\\loops\\epoch\\training_epoch_loop.py:171\u001b[0m, in \u001b[0;36mTrainingEpochLoop.advance\u001b[1;34m(self, data_fetcher)\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(data_fetcher, DataLoaderIterDataFetcher):\n\u001b[0;32m    170\u001b[0m     batch_idx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_idx \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m--> 171\u001b[0m     batch \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(data_fetcher)\n\u001b[0;32m    172\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    173\u001b[0m     batch_idx, batch \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(data_fetcher)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\mri\\lib\\site-packages\\pytorch_lightning\\utilities\\fetching.py:184\u001b[0m, in \u001b[0;36mAbstractDataFetcher.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__next__\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m--> 184\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfetching_function()\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\mri\\lib\\site-packages\\pytorch_lightning\\utilities\\fetching.py:259\u001b[0m, in \u001b[0;36mDataFetcher.fetching_function\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdone:\n\u001b[0;32m    257\u001b[0m     \u001b[39m# this will run only when no pre-fetching was done.\u001b[39;00m\n\u001b[0;32m    258\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 259\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fetch_next_batch(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataloader_iter)\n\u001b[0;32m    260\u001b[0m         \u001b[39m# consume the batch we just fetched\u001b[39;00m\n\u001b[0;32m    261\u001b[0m         batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatches\u001b[39m.\u001b[39mpop(\u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\mri\\lib\\site-packages\\pytorch_lightning\\utilities\\fetching.py:273\u001b[0m, in \u001b[0;36mDataFetcher._fetch_next_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    271\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_fetch_next_batch\u001b[39m(\u001b[39mself\u001b[39m, iterator: Iterator) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    272\u001b[0m     start_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_fetch_start()\n\u001b[1;32m--> 273\u001b[0m     batch \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(iterator)\n\u001b[0;32m    274\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfetched \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    275\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprefetch_batches \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_len:\n\u001b[0;32m    276\u001b[0m         \u001b[39m# when we don't prefetch but the dataloader is sized, we use the length for `done`\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\mri\\lib\\site-packages\\pytorch_lightning\\trainer\\supporters.py:553\u001b[0m, in \u001b[0;36mCombinedLoaderIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    547\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__next__\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    548\u001b[0m     \u001b[39m\"\"\"Fetches the next batch from multiple data loaders.\u001b[39;00m\n\u001b[0;32m    549\u001b[0m \n\u001b[0;32m    550\u001b[0m \u001b[39m    Returns:\u001b[39;00m\n\u001b[0;32m    551\u001b[0m \u001b[39m        a collections of batch data\u001b[39;00m\n\u001b[0;32m    552\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 553\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest_next_batch(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloader_iters)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\mri\\lib\\site-packages\\pytorch_lightning\\trainer\\supporters.py:565\u001b[0m, in \u001b[0;36mCombinedLoaderIterator.request_next_batch\u001b[1;34m(loader_iters)\u001b[0m\n\u001b[0;32m    555\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[0;32m    556\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest_next_batch\u001b[39m(loader_iters: Union[Iterator, Sequence, Mapping]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    557\u001b[0m     \u001b[39m\"\"\"Return the batch of data from multiple iterators.\u001b[39;00m\n\u001b[0;32m    558\u001b[0m \n\u001b[0;32m    559\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    563\u001b[0m \u001b[39m        Any: a collections of batch data\u001b[39;00m\n\u001b[0;32m    564\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 565\u001b[0m     \u001b[39mreturn\u001b[39;00m apply_to_collection(loader_iters, Iterator, \u001b[39mnext\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\mri\\lib\\site-packages\\pytorch_lightning\\utilities\\apply_func.py:99\u001b[0m, in \u001b[0;36mapply_to_collection\u001b[1;34m(data, dtype, function, wrong_dtype, include_none, *args, **kwargs)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[39m# Breaking condition\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, dtype) \u001b[39mand\u001b[39;00m (wrong_dtype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(data, wrong_dtype)):\n\u001b[1;32m---> 99\u001b[0m     \u001b[39mreturn\u001b[39;00m function(data, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    101\u001b[0m elem_type \u001b[39m=\u001b[39m \u001b[39mtype\u001b[39m(data)\n\u001b[0;32m    103\u001b[0m \u001b[39m# Recursively apply to collection items\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\mri\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    528\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    529\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[1;32m--> 530\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    531\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    532\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    533\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    534\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\mri\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:570\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    568\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    569\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 570\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    571\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    572\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\mri\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\mri\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\mri\\lib\\site-packages\\monai\\data\\dataset.py:97\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(index, collections\u001b[39m.\u001b[39mabc\u001b[39m.\u001b[39mSequence):\n\u001b[0;32m     95\u001b[0m     \u001b[39m# dataset[[1, 3, 4]]\u001b[39;00m\n\u001b[0;32m     96\u001b[0m     \u001b[39mreturn\u001b[39;00m Subset(dataset\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m, indices\u001b[39m=\u001b[39mindex)\n\u001b[1;32m---> 97\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_transform(index)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\mri\\lib\\site-packages\\monai\\data\\dataset.py:364\u001b[0m, in \u001b[0;36mPersistentDataset._transform\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    363\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_transform\u001b[39m(\u001b[39mself\u001b[39m, index: \u001b[39mint\u001b[39m):\n\u001b[1;32m--> 364\u001b[0m     pre_random_item \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cachecheck(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata[index])\n\u001b[0;32m    365\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_post_transform(pre_random_item)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\mri\\lib\\site-packages\\monai\\data\\dataset.py:337\u001b[0m, in \u001b[0;36mPersistentDataset._cachecheck\u001b[1;34m(self, item_transformed)\u001b[0m\n\u001b[0;32m    334\u001b[0m         \u001b[39mif\u001b[39;00m sys\u001b[39m.\u001b[39mplatform \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mwin32\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    335\u001b[0m             \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m--> 337\u001b[0m _item_transformed \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_pre_transform(deepcopy(item_transformed))  \u001b[39m# keep the original hashed\u001b[39;00m\n\u001b[0;32m    338\u001b[0m \u001b[39mif\u001b[39;00m hashfile \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    339\u001b[0m     \u001b[39mreturn\u001b[39;00m _item_transformed\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\mri\\lib\\site-packages\\monai\\data\\dataset.py:278\u001b[0m, in \u001b[0;36mPersistentDataset._pre_transform\u001b[1;34m(self, item_transformed)\u001b[0m\n\u001b[0;32m    276\u001b[0m     \u001b[39m# this is to be consistent with CacheDataset even though it's not in a multi-thread situation.\u001b[39;00m\n\u001b[0;32m    277\u001b[0m     _xform \u001b[39m=\u001b[39m deepcopy(_transform) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(_transform, ThreadUnsafe) \u001b[39melse\u001b[39;00m _transform\n\u001b[1;32m--> 278\u001b[0m     item_transformed \u001b[39m=\u001b[39m apply_transform(_xform, item_transformed)\n\u001b[0;32m    279\u001b[0m \u001b[39mreturn\u001b[39;00m item_transformed\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\mri\\lib\\site-packages\\monai\\transforms\\transform.py:89\u001b[0m, in \u001b[0;36mapply_transform\u001b[1;34m(transform, data, map_items, unpack_items, log_stats)\u001b[0m\n\u001b[0;32m     87\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)) \u001b[39mand\u001b[39;00m map_items:\n\u001b[0;32m     88\u001b[0m         \u001b[39mreturn\u001b[39;00m [_apply_transform(transform, item, unpack_items) \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m data]\n\u001b[1;32m---> 89\u001b[0m     \u001b[39mreturn\u001b[39;00m _apply_transform(transform, data, unpack_items)\n\u001b[0;32m     90\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     92\u001b[0m     \u001b[39mif\u001b[39;00m log_stats \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(transform, transforms\u001b[39m.\u001b[39mcompose\u001b[39m.\u001b[39mCompose):\n\u001b[0;32m     93\u001b[0m         \u001b[39m# log the input data information of exact transform in the transform chain\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\mri\\lib\\site-packages\\monai\\transforms\\transform.py:53\u001b[0m, in \u001b[0;36m_apply_transform\u001b[1;34m(transform, parameters, unpack_parameters)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(parameters, \u001b[39mtuple\u001b[39m) \u001b[39mand\u001b[39;00m unpack_parameters:\n\u001b[0;32m     51\u001b[0m     \u001b[39mreturn\u001b[39;00m transform(\u001b[39m*\u001b[39mparameters)\n\u001b[1;32m---> 53\u001b[0m \u001b[39mreturn\u001b[39;00m transform(parameters)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\mri\\lib\\site-packages\\monai\\transforms\\io\\dictionary.py:131\u001b[0m, in \u001b[0;36mLoadImaged.__call__\u001b[1;34m(self, data, reader)\u001b[0m\n\u001b[0;32m    129\u001b[0m d \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(data)\n\u001b[0;32m    130\u001b[0m \u001b[39mfor\u001b[39;00m key, meta_key, meta_key_postfix \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkey_iterator(d, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmeta_keys, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmeta_key_postfix):\n\u001b[1;32m--> 131\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_loader(d[key], reader)\n\u001b[0;32m    132\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_loader\u001b[39m.\u001b[39mimage_only:\n\u001b[0;32m    133\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(data, np\u001b[39m.\u001b[39mndarray):\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\mri\\lib\\site-packages\\monai\\transforms\\io\\array.py:240\u001b[0m, in \u001b[0;36mLoadImage.__call__\u001b[1;34m(self, filename, reader)\u001b[0m\n\u001b[0;32m    232\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    233\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m cannot find a suitable reader for file: \u001b[39m\u001b[39m{\u001b[39;00mfilename\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    234\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m    Please install the reader libraries, see also the installation instructions:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    235\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    236\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m   The current registered: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreaders\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    237\u001b[0m     )\n\u001b[0;32m    239\u001b[0m img_array: NdarrayOrTensor\n\u001b[1;32m--> 240\u001b[0m img_array, meta_data \u001b[39m=\u001b[39m reader\u001b[39m.\u001b[39;49mget_data(img)\n\u001b[0;32m    241\u001b[0m img_array \u001b[39m=\u001b[39m img_array\u001b[39m.\u001b[39mastype(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtype, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    242\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(meta_data, \u001b[39mdict\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\mri\\lib\\site-packages\\monai\\data\\image_reader.py:450\u001b[0m, in \u001b[0;36mNibabelReader.get_data\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m    448\u001b[0m     header[\u001b[39m\"\u001b[39m\u001b[39maffine\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_affine(i)\n\u001b[0;32m    449\u001b[0m header[\u001b[39m\"\u001b[39m\u001b[39mspatial_shape\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_spatial_shape(i)\n\u001b[1;32m--> 450\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_array_data(i)\n\u001b[0;32m    451\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msqueeze_non_spatial_dims:\n\u001b[0;32m    452\u001b[0m     \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(data\u001b[39m.\u001b[39mshape), \u001b[39mlen\u001b[39m(header[\u001b[39m\"\u001b[39m\u001b[39mspatial_shape\u001b[39m\u001b[39m\"\u001b[39m]), \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\mri\\lib\\site-packages\\monai\\data\\image_reader.py:522\u001b[0m, in \u001b[0;36mNibabelReader._get_array_data\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m    514\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_array_data\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[0;32m    515\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    516\u001b[0m \u001b[39m    Get the raw array data of the image, converted to Numpy array.\u001b[39;00m\n\u001b[0;32m    517\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m \n\u001b[0;32m    521\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 522\u001b[0m     _array \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49marray(img\u001b[39m.\u001b[39;49mget_fdata(dtype\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdtype))\n\u001b[0;32m    523\u001b[0m     img\u001b[39m.\u001b[39muncache()\n\u001b[0;32m    524\u001b[0m     \u001b[39mreturn\u001b[39;00m _array\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.tune(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "def clear_cache_dir():\n",
    "    cwd = os.getcwd()\n",
    "    shutil.rmtree(os.path.join(cwd, 'cache'))\n",
    "    \n",
    "clear_cache_dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('mri')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ae52af71424788817ac7931c57470275392053099e15f5f167044ac7f80a887f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
